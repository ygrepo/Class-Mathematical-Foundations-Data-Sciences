\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage[psamsfonts]{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{mathtools}


\input{macros}

\begin{document}

\noindent DS-GA.1013 Mathematical Tools for Data Science \\
Homework 1 \\
Yves Greatti - yg390\\


\begin{enumerate}
\item (Rotation) For a symmetric matrix $A$, can there be a nonzero vector $x$ such that $Ax$ is nonzero and orthogonal to $x$? Either prove that this is impossible, or explain under what condition on the eigenvalues of $A$ such a vector exists.
Let $x \in V, x \neq 0$, an inner product space , by the spectral theorem there exists an orthonormal basis of  $V$, consisting of eigenvectors of $A$, let $u_1$, \ldots, $u_n$ be the eigenbasis of $A$, and $\lambda_1, \ldots, \lambda_n$ the eigenvalues for each of these eigenvectors.
$x \in \text{span}\{u_1, \ldots, u_n \} \Rightarrow  x=\sum_{i=1,n} \alpha_i u_i, \alpha_i \neq 0$.  $x^T (Ax) = (\sum_{i=1,n} \alpha_i u_i) (\sum_{j=1,n} \alpha_j A u_j) =  (\sum_{i=1,n} \alpha_i u_i) (\sum_{j=1,n} \alpha_j \lambda_j u_j) = \sum_{i=1,n} \alpha_i^2 \lambda_i$ since $u_i^T u_j = 0$ for $i \neq j$ and $u_i^T u_i =1$. $Ax$ is orthogonal to $x$: $x^T (Ax) = 0 \Rightarrow  \sum_{i=1,n} \alpha_i^2 \lambda_i = 0$.

\item (Matrix decomposition) The trace can be used to define an inner product between matrices:
\begin{align}
\PROD{A}{B} := \trace{A^TB}, \quad A,B \in \R^{m \times n},
\end{align}
where the corresponding norm is the Frobenius norm $\normF{A}:=\PROD{A}{A}$.
\begin{enumerate}
\item Express the inner product in terms of vectorized matrices and use the result to prove that this is a valid inner product.
$(A B)_{ij} = (\sum_k A_{ik} B_{kj})_{ij}$, and $(A^TB) _{ij} = (\sum_k A_{ki} B_{kj})_{ij}$.
$\trace{A} = \sum_i A_{ii}  \Rightarrow \trace{A^TB} = \sum_i \sum_k A_{ki} B_{ki} = \sum_i \sum_j A_{ij} B_{ij} = \text{vec}(A)^T \text{vect}(B) = \PROD{\text{vec}(A)} {\text{vec}(B)}$.
The trace is then the inner product between vectors in $\R^{mn}$ thus is a valid inner product.

\item Prove that for any $A,B \in \R^{m \times n}$, $\trace{A^TB}=\trace{BA^T}$.
$\trace{BA^T} =  \sum_i \sum_k B_{ik} A_{ik} =  \sum_i \sum_j A_{ij} B_{ij} =  \trace{A^TB}$.

\item Let $u_1$, \ldots, $u_n$ be the eigenvectors of a symmetric matrix $A$. Compute the inner product between the rank-1 matrices $u_iu_i^T$ and $u_ju_j^T$ for $i \neq j$, and also the norm of $u_iu_i^T$ for $i=1,\ldots,n$. 
For $i \neq j$, $\PROD{u_iu_i^T}{u_ju_j^T} = \trace{u_iu_i^Tu_ju_j^T} = 0$, if $u_i, u_j$ are two eigenvectors for different eigenvalues.
if $i=j$ then  $\PROD{u_iu_i^T}{u_iu_i^T} = \trace{u_iu_i^Tu_iu_i^T} = \trace{(u_i^Tu_i)^2} = (u_i^Tu_i)^2 \Rightarrow |\normF{u_i u_i^T} =  u_i^Tu_i$ and $\normF{u_iu_i^T}=1$ if the eigenvectors are also orthonormal.
\item What is the projection of $A$ onto $u_iu_i^T$?
The projection of $A$ onto $u_iu_i^T$ is $\PROD{A}{u_iu_i^T}$. If $A$ is a symmetric matrix, by the spectral theorem, $A=U D U^T= \sum_i \lambda_i u_i u_i^T$ where $\lambda_i, i=1, \ldots,n$ are the eigenvalues of $A$. If $u_1$, \ldots, $u_n$ form an eigenbasis then $\PROD{A}{u_iu_i^T} = \lambda_i$. $u_iu_i^T$ is the matrix for the orthogonal projection
onto $\text{ span }(u_i)$.

\item Provide a geometric interpretation of the matrix $A':=A-\lambda_1 u_1u_1^T$, which we defined in the proof of the spectral theorem, based on your previous answers.
From the previous question the orthogonal projection of A in $u_iu_i^T$ is $\lambda_i u_iu_i^T$ so $A' = \sum_i \lambda_i u_iu_i^T, i \neq 1$ has row or column subspaces contained in  $(u_1)^\bot$.

\end{enumerate}

\item (Quadratic forms) Let $A\in \R^{n \times n}$ be a symmetric matrix, and let $f(x):=x^TAx$ be the corresponding quadratic form. We consider the 1D function $g_{v}(t)=f(tv)$ obtained by restricting the quadratic form to lie in the direction of a vector $v$ with unit $\ell_2$ norm.
\begin{enumerate}
\item Is $g_{v}(t)$ a polynomial? If so, what kind?
	$g_{v}(t) = f(tv) = (tv)^T A (tv) = t^2 v^T A v = v^T A v \; t^2$, $v^T A v$ is a scalar, and $g_{v}(t)$ is a second-order polynomial in $t$. 
\item What is the curvature (i.e. the second derivative) of $g_{v}(t)=f(tv)$ at an arbitrary point $t$?
$g'_{v}(t)= 2 v^T A v \; t$ and the curvature is $g''_{v}(t)= 2 v^T A v$

\item What are the directions of maximum and minimum curvature of the quadratic form? What are the corresponding curvatures equal to?
By the spectral theorem, $A = U \textbf{diag}(\lambda) U^T$ where $\textbf{diag}$ is the diagonal matrix with on the diagonal: $\lambda_1 \ge \lambda_2 \ge \ldots \lambda_n$, which are the eigenvalues and $u_1, \dots, u_n$ the corresponding eigenvectors. The largest eigenvalue is
$\lambda_1 = \max_{\|v\|_2 =1} v^T A v$ with eigenvector $u_1 = \arg \max_{\|v\|_2 =1} v^T A v$, and the smaller eigenvalue  is given by $\lambda_n = \max_{\|v\|_2 =1} v^T A v, u_n = \arg \max_{\|v\|_2 =1} v^T A v$. Thus the maximum curvature is given by the largest eigenvalue $\lambda_1$ and  is in the direction of the corresponding eigenvector $u_1$. 
	The smallest curvature is  given  by  the  smallest  eigenvalue $\lambda_n$ and is in the direction of the corresponding eigenvector $u_n$.

\end{enumerate}

\item (Projected gradient ascent) Projected gradient descent is a method designed to find the maximum of a differentiable function $f:\R^n \rightarrow \R$ in a constraint set $\ml{S}$. Let $\ml{P}_{\ml{S}}$ denote the projection onto $\ml{S}$, i.e.
\begin{align}
\ml{P}_{\ml{S}}(x) := \arg \min_{y \in \ml{S}} \normTwo{x-y}^2.
\end{align} 
The $k$th update of projected gradient ascent equals
\begin{align}
x^{[k]} :=\ml{P}_{\ml{S}}( x^{[k-1]} + \alpha \nabla f (x^{[k-1]}) ), \qquad k=1,2,\ldots,
\end{align}
where $\alpha$ is a positive constant and $x^{[0]}$ is an arbitrary initial point.
\begin{enumerate}
\item Use the same arguments we used to prove Lemmas 5.1 and 5.2 in the notes on PCA to derive the projection of a vector $x$ onto the unit sphere in $n$ dimensions.
Let define $f(x) =  \normTwo{x-y}^2,y \in \ml{S}$, the directional derivative cannot be different than zero $f'_v(x) = \PROD{\nabla{x}}{v} = 0$ for any $v$ such that $x+ \epsilon v$ is on the sphere $\ml{S}$. Let $g(x)=x^T x, \| y \|_2 = 1$, $g$ describes points on the surface of the unit sphere. $x +v$ is in the tangent  plane of $g$ at $x$ if $\nabla{g(x)}^T v = 0$, and for $\epsilon \approx 0$, $g(x + \epsilon v) \approx g(x)$. So we are looking for global minimizer points where the level curves of $f$ are tangent to the curve $g$, or where the gradients are colinear. $\nabla_x{f(x)} = \nabla_x{(x^T x -2 x^T y + y^T y)} = 2 (x-y)$ and $\nabla_x{g(x)} = 2 x$, so the projection of $x$ on $\ml{S}$, $y_p$, verifies $x -y_p = \lambda x$ or $y_p = (1- \lambda) x$. for any vector $y$, we have $y = (1-\lambda) x + x_\bot$ where $x_\bot$ is in the hyperplane orthogonal to $x$. We want to show that the projection point is the closest to $x$.
	By Pythagorasâ€™ theorem, $\|y\|_2^2 = (1-\lambda)^2 \|x\|^2 + \|x_\bot\|^2$ and:
	\begin{align*}
		\|y - x \|_2^2	&=	\|y\|_2^2 -2 y^T x + \| x \|_2^2 \\
		y^Tx			&=	((1-\lambda) x^T + x_\bot^T) x \\
					&=	(1-\lambda) x^T x \Rightarrow \\
		\|y - x \|_2^2	&=	\|y\|_2^2 - 2 (1-\lambda) \|x\|_2^2 + \| x\|_2^2 \\
					&= 	(1-\lambda)^2 \|x\|^2 + \|x_\bot\|^2  - 2 (1-\lambda) \|x\|_2^2 + \| x\|_2^2 \\
					&=	\lambda^2 \| x \|_2^2 + \|x_\bot\|^2 \\	
					&> \| x - y_p \|_2^2
	\end{align*}
	Thus  $\arg \min_{y \in \ml{S}} \normTwo{x-y}^2 =\arg \min  (1-\lambda)^2 \|x\|_2^2, \lambda x  \in \ml{S}$. If $x \in  \ml{S}$ then $\lambda = 1$, 
	if $x \neq \ml{S}$ thus $\lambda x  \in \ml{S} \Rightarrow \|\lambda x \|_2 = 1 \Rightarrow \lambda = \frac{1}{\|x\|_2}$, and $\lambda = \min(1,  \frac{1}{\|x\|_2})$,
	that is $\ml{P}_{\ml{S}}(x) = \min(x, \frac{x}{\|x\|_2})$.


\item Derive an algorithm based on projected gradient ascent to find the maximum eigenvalue of a symmetric matrix $A\in \R^{n \times n}$.
Let $f(x) = x^T A x$, the largest eigenvalue can be found by solving the optimization problem $\lambda_1= \max _{\|x\|_2=1} x^T A x$ or equivalently $\lambda_1 = \min_{\|x\|_2=1}  - f(x)$.
We have $\nabla{f(x)} = 2 A x$, by assumption and using the previous result, the algorithm to find the largest eigenvalue of a symmetric matrix $A\in \R^{n \times n}$ is:

\begin{align*}
	x^{'[k-1]}	&= x^{[k-1]} + \alpha \nabla{f(x^{[k-1]} )} \\
			&= x^{[k-1]} -2 \alpha A x \\
	x^{[k]}	&= \frac{x^{'[k-1]}}{\|x^{'[k-1]}\|_2}	
\end{align*}


\item Let us express the iterations in the basis of eigenvectors of $A$: $x^{[k]} := \sum_{i=1}^{n}\beta_i^{[k]} u_i$. Compute the ratio between the coefficient corresponding to the largest eigenvalue and the rest $\frac{\beta_1^{[k]}}{\beta_i^{[k]}}$ as a function of $k$, $\alpha$, and $\beta_1^{[0]}$, \ldots, $\beta_n^{[0]}$. Under what conditions on $\alpha$ and the initial point does the algorithm converge to the eigenvector $u_1$ corresponding to the largest eigenvalue? What happens if $\alpha$ is extremely large (i.e. when $\alpha \rightarrow \infty$)?
\item Implement the algorithm derived in part (b). Support code is provided in {\tt main.py} within {\tt Q4.zip}. Observe what happens for different sizes of $\alpha$. Report the plots generated by the script.
\end{enumerate}

\end{enumerate}

\end{document}
