\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage[psamsfonts]{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{mathtools}


\input{macros}

\begin{document}

\noindent DS-GA.1013 Mathematical Tools for Data Science \\
Homework 1 \\
Yves Greatti - yg390\\


\begin{enumerate}
\item (Rotation) For a symmetric matrix $A$, can there be a nonzero vector $x$ such that $Ax$ is nonzero and orthogonal to $x$? Either prove that this is impossible, or explain under what condition on the eigenvalues of $A$ such a vector exists.
Let $x \in V$, an inner product space , by the spectral theorem there exists an orthonormal basis of  $V$, consisting of eigenvectors of $A$, let $u_1$, \ldots, $u_n$ be the eigenvectors of $A$, and $\lambda_i, \ldots, \lambda_n$ the eigenvalues for each of these eigenvectors.
$x \in \text{span}\{u_1, \ldots, u_n \} \Rightarrow  x=\sum_{i=1,n} \alpha_i u_i, \alpha_i \neq 0$.  $x^T (Ax) = (\sum_{i=1,n} \alpha_i u_i) (\sum_{j=1,n} \alpha_j A u_j) =  (\sum_{i=1,n} \alpha_i u_i) (\sum_{j=1,n} \alpha_j \lambda_j u_j) = \sum_{i=1,n} \alpha_i^2 \lambda_i$ since $u_i^T u_j = 0$ for $i \neq j$ and $u_i^T u_i =1$. $x$ and $Ax$ are nonzero and $Ax$ is orthogonal to $x$: $x^T (Ax) = 0 \Rightarrow  \sum_{i=1,n} \alpha_i^2 \lambda_i = 0$.

\item (Matrix decomposition) The trace can be used to define an inner product between matrices:
\begin{align}
\PROD{A}{B} := \trace{A^TB}, \quad A,B \in \R^{m \times n},
\end{align}
where the corresponding norm is the Frobenius norm $\normF{A}:=\PROD{A}{A}$.
\begin{enumerate}
\item Express the inner product in terms of vectorized matrices and use the result to prove that this is a valid inner product.
$(A B)_{ij} = (\sum_k A_{ik} B_{kj})_{ij}$, and $(A^TB) _{ij} = (\sum_k A_{ki} B_{kj})_{ij}$.
$\trace{A} = \sum_i A_{ii}  \Rightarrow \trace{A^TB} = \sum_i \sum_k A_{ki} B_{ki} = \sum_i \sum_j A_{ij} B_{ij} = \text{vec}(A)^T \text{vect}(B) = \PROD{\text{vec}(A)} {\text{vec}(B)}$.
The trace is then the inner product between vectors in $\R^{m \times n}$ thus is a valid inner product.

\item Prove that for any $A,B \in \R^{m \times n}$, $\trace{A^TB}=\trace{BA^T}$.
$\trace{BA^T} =  \sum_i \sum_k B_{ik} A_{ik} =  \sum_i \sum_j A_{ij} B_{ij} =  \trace{A^TB}$.

\item Let $u_1$, \ldots, $u_n$ be the eigenvectors of a symmetric matrix $A$. Compute the inner product between the rank-1 matrices $u_iu_i^T$ and $u_ju_j^T$ for $i \neq j$, and also the norm of $u_iu_i^T$ for $i=1,\ldots,n$. 
For $i \neq j$, $\PROD{u_iu_i^T}{u_ju_j^T} = \trace{u_iu_i^Tu_ju_j^T} = 0$ since $u_i \bot u_j$, $u_i, u_j$ being two eigenvectors for different eigenvalues of the symmetric matrix $A$.
if $i=j$ then  $\PROD{u_iu_i^T}{u_iu_i^T} = \trace{u_iu_i^Tu_iu_i^T} = \trace{(u_i^Tu_i)^2} = (u_i^Tu_i)^2 \Rightarrow |\normF{u_i u_i^T} =  u_i^Tu_i$.
\item What is the projection of $A$ onto $u_iu_i^T$?
The projection of $A$ onto $u_iu_i^T$ is $\PROD{A}{u_iu_i^T}$, A is a symmetric matrix, by the spectral theorem, $A=U D U^T$ where $D=\textbf{diag}(\lambda)$.
$\PROD{A}{UU^T} = \trace{U D U^T UU^T} = \trace{UU^T (U D U^T)} = \trace{UU^TUU^TD} = \trace{(U^TU)^2D}$
thus $\PROD{A}{u_iu_i^T}=\lambda_i (u_i^Tu_i)$.
\item Provide a geometric interpretation of the matrix $A':=A-\lambda_1 u_1u_1^T$, which we defined in the proof of the spectral theorem, based on your previous answers.
From the previous question the orthogonal projection of A in $u_iu_i^T$ is $\lambda_i u_iu_i^T$ so $A'$ has row or column subspaces contained in  $(u_1)^\bot$.
% confirmed by $(A-\lambda_1 u_1u_1^T)u_1 = A u_1 -\lambda_1u_1 = 0$.

\end{enumerate}

\item (Quadratic forms) Let $A\in \R^{n \times n}$ be a symmetric matrix, and let $f(x):=x^TAx$ be the corresponding quadratic form. We consider the 1D function $g_{v}(t)=f(tv)$ obtained by restricting the quadratic form to lie in the direction of a vector $v$ with unit $\ell_2$ norm.
\begin{enumerate}
\item Is $g_{v}(t)$ a polynomial? If so, what kind?
\item What is the curvature (i.e. the second derivative) of $g_{v}(t)=f(tv)$ at an arbitrary point $t$?
\item What are the directions of maximum and minimum curvature of the quadratic form? What are the corresponding curvatures equal to?
\end{enumerate}

\item (Projected gradient ascent) Projected gradient descent is a method designed to find the maximum of a differentiable function $f:\R^n \rightarrow \R$ in a constraint set $\ml{S}$. Let $\ml{P}_{\ml{S}}$ denote the projection onto $\ml{S}$, i.e.
\begin{align}
\ml{P}_{\ml{S}}(x) := \arg \min_{y \in \ml{S}} \normTwo{x-y}^2.
\end{align} 
The $k$th update of projected gradient ascent equals
\begin{align}
x^{[k]} :=\ml{P}_{\ml{S}}( x^{[k-1]} + \alpha \nabla f (x^{[k-1]}) ), \qquad k=1,2,\ldots,
\end{align}
where $\alpha$ is a positive constant and $x^{[0]}$ is an arbitrary initial point.
\begin{enumerate}
\item Use the same arguments we used to prove Lemmas 5.1 and 5.2 in the notes on PCA to derive the projection of a vector $x$ onto the unit sphere in $n$ dimensions.
\item Derive an algorithm based on projected gradient ascent to find the maximum eigenvalue of a symmetric matrix $A\in \R^{n \times n}$.
\item Let us express the iterations in the basis of eigenvectors of $A$: $x^{[k]} := \sum_{i=1}^{n}\beta_i^{[k]} u_i$. Compute the ratio between the coefficient corresponding to the largest eigenvalue and the rest $\frac{\beta_1^{[k]}}{\beta_i^{[k]}}$ as a function of $k$, $\alpha$, and $\beta_1^{[0]}$, \ldots, $\beta_n^{[0]}$. Under what conditions on $\alpha$ and the initial point does the algorithm converge to the eigenvector $u_1$ corresponding to the largest eigenvalue? What happens if $\alpha$ is extremely large (i.e. when $\alpha \rightarrow \infty$)?
\item Implement the algorithm derived in part (b). Support code is provided in {\tt main.py} within {\tt Q4.zip}. Observe what happens for different sizes of $\alpha$. Report the plots generated by the script.
\end{enumerate}

\end{enumerate}

\end{document}
