\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}


\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}



\input{macros}

\begin{document}

\begin{center}
{\large{\textbf{Homework 11}} } \vspace{0.2cm}\\
Due May 10 at 11 pm
\end{center}
Yves Greatti - yg390\\

\begin{enumerate}

\item (Lasso and $\ell_0$)
The file \texttt{X.txt} contains a $50 \times 300$ matrix $X$,
  and the file \texttt{y.txt} contains the $50\times 1$ vector
  $y$.  Each line of each file represents a row of the corresponding
  matrix, and the values on each line are space-delimited.
  \begin{enumerate}
  \item Consider the lasso problem
    $$\min_{\beta} \frac{1}{2n}\|X\beta -y\|^2 + \lambda\|\beta\|_1$$
    where $\lambda>0$ is a parameter and $n=50$.  Construct a (semilogx) plot
    that draws a
    separate path for each coefficient value as a function
    of~$\lambda$.
    Include values of $\lambda$ between $0.01$ and $2$ (you can
    include more if you want),
    and make your values spaced evenly on the log axis (e.g.,
    \texttt{np.geomspace}).  You can solve the lasso problem using
    whatever code/library you want.
  \item Determine the minimizer of
    $$\begin{array}{ll}
    \text{minimize} & \|\beta\|_0\\
    \text{subject to} & X\beta = y.
    \end{array}$$
    
    Assume that the minimizer has small $\ell_0$ norm, i.e $\ell_0 \leq 2$. Explain your strategy and justify that it finds the minimizer.  
    Report the nonzero coefficients of the minimizer, and
    their values.  Remember that two floating point values may be different for numerical reasons even if they represent the same value.
    \item Will your strategy in (b) always find the optimal minimizer of any least-squares problem with $\ell_0$ regularization?
  \end{enumerate}
  
 \newpage
 \item (Proximal operator) The proximal operator of a function $f: \R^n \rightarrow \R$ is defined as
\begin{align}
\op{prox}_{f}\brac{y} := \arg \min_{x} f\brac{x}+ \frac{1}{2} \normTwo{x-y}^2.
\end{align}
  \begin{enumerate}
  \item Derive the proximal operator of the squared $\ell_2$ norm weighted by a constant $\alpha > 0$, i.e. $f(x)=\alpha \normTwo{x}^2$.

\begin{align*}
\op{prox}_{f}\brac{y} := \arg \min_{x}  \alpha \normTwo{x}^2 + \frac{1}{2} \normTwo{x-y}^2
\end{align*}

The two terms are quadratic, therefore differentiable, the gradient is 
$$
	\nabla_x \op{prox}_{f}\brac{y} = 2 \alpha x + (x-y)
$$
 Setting the gradient to zero, yields:
\begin{align*}
	2 \alpha x + (x-y)			&= 	0 \\
		x 					&= \frac{1} {1+2 \alpha} y \\
		\op{prox}_{f}\brac{y} 		&=	 \frac{1} {1+2 \alpha} y, \alpha > 0 \\
\end{align*}
 
  \item Prove that the proximal operator of the $\ell_1$ norm weighted by a constant $\alpha > 0$ is a soft-thresholding operator,
\begin{align}
\op{prox}_{\alpha \, \normOne{ \cdot }}\brac{ y } & = \ml{S}_{\alpha}\brac{y},
\end{align}
where 
\begin{align}
\ml{S}_{\alpha}\brac{y}[i] := 
\begin{cases}
y\,[i] - \op{sign}\brac{y\,[i]} \alpha  \qquad & \text{if $\abs{y\,[i]} \geq \alpha$}, \\
0 \quad & \text{otherwise}.
\end{cases}
\end{align}\\

$$
\op{prox}_{\alpha \, \normOne{ \cdot }}\brac{ y } = \alpha \|x\|_1 + \frac{1}{2} \normTwo{x-y}^2, \alpha > 0
$$
And we are looking for:
\begin{align*}
	0 \in \partial( \alpha \|x\|_1) + \nabla_x (\frac{1}{2} \normTwo{x-y}^2) \\
	0 \in \alpha \; \partial( \|x\|_1) + (x-y) \\
\end{align*}
We examine each component of $x$ and $y$ separately.
Assume first that $x[i] \neq 0$ then $ \partial( \|x\|_1) = \sign{x[i]}$, setting the subgradient to $0$, we have:
\begin{align*}
	x[i] - y[i] + \alpha \sign{x[i]}	&= 0 \\
	x[i]					&= y[i] - \alpha \sign{x[i]} \\
\end{align*}
Note that
\begin{align*}
	x[i] < 0, \sign{x[i]} = -1 &\rightarrow y[i] + \alpha < 0 \text{ ~ or }  y[i] < -\alpha < 0 \\
	x[i] > 0, \sign{x[i]} = 1 &\rightarrow y[i] - \alpha > 0 \text{ ~ or }  y[i] > \alpha > 0 \\
\end{align*}
thus in this case $\sign{x[i]} = \sign{y[i]}$ and the optimal point is $y[i] - \alpha \sign{y[i]}$.
In the case where $x[i]=0$, let $\gamma =  \partial(\|x\|_1), | \gamma | \le 1$ then it holds
\begin{align*}
	x[i] - y[i] + \alpha \gamma = 0 &\rightarrow y[i] - \alpha \gamma = 0 \\
							&	y[i] = \gamma \alpha \\
							& |y[i]| \le \alpha
\end{align*}
Putting all together, we get
\begin{align}
\op{prox}_{\alpha \, \normOne{ \cdot }}\brac{ y } = 
\begin{cases}
y\,[i] - \op{sign}\brac{y\,[i]} \alpha  \qquad & \text{if $\abs{y\,[i]} \geq \alpha$}, \\
0 \quad & \text{otherwise}.
\end{cases}
\end{align}

    \item Prove that if $X \in \R^{p \times n}$ has orthonormal rows ($p \leq n$) and $y \in \R^{n}$, then for any function $f$
\begin{align}
\arg \min_{\beta} \frac{1}{2} \normTwo{ y - X^T\beta }^2 + f(\beta) = \arg \min_{\beta} \frac{1}{2} \normTwo{ Xy - \beta }^2 + f(\beta).
\end{align}
The two expressions for the same function $f$ differs on the first term, so we want to show that
$$
\arg \min_{\beta} \frac{1}{2} \normTwo{ y - X^T\beta }^2  =  \arg \min_{\beta} \frac{1}{2} \normTwo{ Xy - \beta }^2
$$

\item Use the answers to the previous questions to compare the ridge-regression and lasso estimators for a regression problem where the features are orthonormal.
  \end{enumerate}
The use of $l_1$, $l_2$ norms gives rise to the problems, for $\lambda > 0$:

\begin{align*}
\frac{1}{2} \argmin_\beta \| y - X^T \beta\|_2^2 + \lambda \|\beta\|_2^2	& \text{~ Ridge regression} \\
\frac{1}{2} \argmin_\beta \| y - X^T \beta\|_2^2 + \lambda \|\beta\|_1	& \text{~ Lasso regression}
\end{align*} 
which is equivalent from part c) to
\begin{align*}
 \argmin_\beta  \lambda \|\beta\|_2^2 + \frac{1}{2} \| \beta - X y \|_2^2 	& \text{~ Ridge regression} \\
 \argmin_\beta   \lambda \|\beta\|_1+    \frac{1}{2} \| \beta - X y \|_2^2 	& \text{~ Lasso regression}
\end{align*} 
And for part a) and b), the solutions of these two problems are the proximal operators:
\begin{align*}
	\beta_{\text{ridge}}	&= 	 \frac{1} {1+2 \lambda} X y \\
	\beta_{\text{lasso}}	&= 	  \ml{S}_{\lambda}\brac{X y}\\
\end{align*} 

 \newpage
  \item (Proximal gradient method)   
 \begin{enumerate}
 \item The first-order approximation to a function $f:\R^{p}\rightarrow \R$ at $x \in \R^{p}$ equals
 \begin{align}
f\brac{x} + \nabla f\brac{x}^T\brac{y-x}. 
\end{align}
We want to minimize this first-order approximation locally. To this end we fix a real constant $\alpha >0$ and augment the approximation with an $\ell_2$-norm term that keeps us close to $x$,
 \begin{align}
f_{x}(y) :=f\brac{x} + \nabla f\brac{x}^T\brac{y-x} +  \frac{1}{2 \, \alpha }\normTwo{y-x}^2.
\end{align}
 Prove that the minimum of $f_{x}$ is the gradient descent update $x- \alpha \nabla f\brac{x}$.
\item 
Inspired by the previous question, how would you modify gradient descent to minimize a function of the form
\begin{align}
h(x) = f_1(x) + f_2(x),
\end{align}
where $f_1$ is differentiable, and $f_2$ is nondifferentiable but has a proximal operator that is easy to compute?
\item Show that a vector $x^{\ast}$ is a solution to
\begin{align}
 \op{minimize} \quad & f_1\brac{x} + f_2\brac{x},
\end{align}
where $f_1$ is differentiable, and $f_2$ is nondifferentiable, if and only if it is a fixed point of the iteration you proposed in the previous question for any $\alpha > 0$.
  \end{enumerate}
  
  \item (Iterative shrinkage-thresholding algorithm)   
 \begin{enumerate}
 \item What is the proximal gradient update corresponding to the lasso problem defined below? Your answer will involve a hyperparameter which we will call as $\alpha$. 
 \begin{align*}
 \frac{1}{2} \normTwo{y - X\beta}^2 + \lambda |\beta|_1
 \end{align*}
 \item How would you check whether you have reached an optimum? How would you modify this to take into account possible numerical inaccuracies?
 \item Implement the method and apply it to the problem in \texttt{pgd\_lasso-question.ipynb}. You have to fill in blocks of code corresponds to the proximal gradient update step and termination condition. Report all the generated plots.
 \end{enumerate}
 

 \end{enumerate}
\end{document}
