\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}

\input{macros}

\begin{document}

\begin{center}
{\large{\textbf{Homework 6}} } \vspace{0.2cm}\\
Due April 5 at 11 pm
\end{center}
Yves Greatti - yg390\\

\begin{enumerate}

\item (Gradient descent and ridge regression) In this problem we study the iterations of gradient descent applied to the ridge-regression cost function
\begin{align}
\min_{\beta \in \R^p} \frac{1}{2}\normTwo{y-X^T\beta}^2 + \frac{\lambda}{2}\normTwo{\beta}^2,
\end{align}
where $X \in \R^{p \times n}$ is a fixed feature matrix and $y\in \R^{n}$ is a response vector. (The factor of 1/2 is just there to make calculations a bit cleaner.)
\begin{enumerate}
\item Derive a closed form expression for the value of the estimated coefficient $\beta^{\brac{k}}$ at the $k$th iteration of gradient descent initialized at the origin in terms of the SVD of $X$ when the step size is constant. \\

The gradient-descent updates are:
\begin{align*}
		\beta^{(k+1)}	&=	\beta^{(k)} - \; \alpha_k \nabla_{\beta} f(\beta^{(k)})	\\
					&=	\beta^{(k)}  - \alpha_k (X X^T \beta^{(k)} - X y +  \lambda \beta^{(k)}) \\
					&=    ((1- \lambda \alpha_k) I  - \alpha_k X X^T) \beta^{(k)}  +  \alpha_k  X y  \\
					&=	\big( ( 1 - \alpha \lambda) I - \alpha X X^T  \big)^{k+1} \beta^{(0)}  +	\alpha \sum_{i=0}^k \big( (1 - \alpha \lambda) I - \alpha X X^T \big)^i X y \\
					&=	\alpha \sum_{i=0}^k \big( (1 - \alpha \lambda) I - \alpha X X^T \big)^i X y \
\end{align*}
since the step size $\alpha_k = \alpha$ is constant and $\beta^{(0)}$ is the zero vector (initialization at the origin).    
Let the svd of $X = U S V^T$ then 
\begin{align*}
		\beta^{(k+1)}	&= \alpha  \sum_{i=0}^k \big( (1 - \alpha \lambda) I - \alpha U S^2 U^T  \big)^i U S V^T y \\
\end{align*}

Assuming $p \le n$ and $X$ is full rank, $UU^T = U^T U = I$ and we have:
\begin{align*}
	\beta^{(k+1)}	&=  \alpha  \sum_{i=0}^k \big( (1  - \alpha \lambda) UU^T- \alpha U S^2 U^T \big)^i U S V^T y \\
				&= \alpha U   \sum_{i=0}^k \big( ( 1 - \alpha \lambda) I - \alpha S^2  \big)^i  S V^T y \\
				&=  \alpha U   \text{diag}_{j=1}^p \sum_{i=0}^k \big(  1 - \alpha (s_j^2 + \lambda)  \big)^i  S V^T y \\
				&=  U   \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k+1} s_j}{s_j^2 + \lambda}  \;  V^T y \\ 
\end{align*}

\item Under what condition on the step size does gradient descent converge to the ridge-regression coefficient estimate as $k \rightarrow \infty$?\\
If step size $\alpha$ is small enough: $0 < \alpha < \frac{2} {\lambda + s_1^2} \le \frac{2} {\lambda + s_j^2} \rightarrow |1 - \alpha (s_j^2 + \lambda)| < 1$ then
 $\lim_{k \rightarrow \infty} (1 - \alpha (s_j^2 + \lambda))^k =0, j=1, \dots ,p $, gradient descent converges to:
\begin{align*}
	\lim_{k \rightarrow \infty} \beta^{(k)}	&= U  \;  \text{diag}_{j=1}^p \bigg( \frac{s_j} {s_j^2 + \lambda} \bigg)  V^T y \\
									&= U (S^2 + \lambda I)^{-1} S V^T y
\end{align*}
which are the ridge-regression coefficient estimates.

\item Assume the following additive model for the data:  
\begin{align}
\ry_{\op{train}} := X^T \beta_{\op{true}} + \rnd{z}_{\op{train}},
\end{align}
where $\rnd{z}_{\op{train}}$ is modeled as an $n$-dimensional iid Gaussian vector with zero mean and variance $\sigma^2$. What is the distribution of the estimated coefficient $\rnd{\beta}^{\brac{k}}$ at the $k$th iteration of gradient descent initialized at the origin?\\
Using the expression of the estimated coefficients from part a, we now have:

\begin{align*}
	\rnd{\beta}^{\brac{k}} &=  U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j}{s_j^2 + \lambda}  \;  V^T (X^T \beta_{\op{true}} + \rnd{z}_{\op{train}} )\\
					&=  U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j}{s_j^2 + \lambda}  \;  V^T (V S U^T \beta_{\op{true}} + \rnd{z}_{\op{train}} )\\
					&=  U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j^2}{s_j^2 + \lambda}  \; U^T \beta_{\op{true}} 
					+      U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j}{s_j^2 + \lambda}  \;  V^T  \rnd{z}_{\op{train}} \\
\end{align*}
Using theorem 8,6 from the notes on PCA, then  the estimated coefficient $\rnd{\beta}^{\brac{k}}$ at the $k$th iteration of gradient descent initialized at the origin is a Gaussian random vector  with mean:

$$ \beta_{\text{GD}}  = \sum_{j=1}^p  \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j^2}{s_j^2 + \lambda}  \PROD{u_j}{\beta_{\op{true}}} u_j $$
and covariance matrix

$$ \Sigma_{\text{GD}} = \sigma^2 U  \text{diag}_{j=1}^p \frac{ (1 - ( 1 - \alpha (s_j^2 + \lambda))^{k})^2 s_j^2}{(s_j^2 + \lambda)^2}  U^T $$

\item Complete the script \emph{RR\_GD\_landscape.py} in order to verify your answer to the previous question. Report the figures generated by the script.\\

The figures for the mean and covariance matrix derived in part b, are reported below. We can see as k increases ($k=300$), the estimated coefficients have a distribution similar to the ones reported for the ridge-regression estimated coefficients
for the same value of $\lambda=0.05$ in the PCA notes (figure 16).  
For small value of $k$, we have a bias: the estimated coefficients are shifting towards the initial starting point of gradient descent, the origin.
Since $\alpha=.2 < \frac{2}{\lambda + s_1^2} =   \frac{2}{0.05+ 1} \approx 1.9$ so as the number of steps  $k$ increases, the variance increases and is proportional to $\sigma^2 \frac{s_i^2}{(s_i^2 + \lambda)^2}$.
The eigenvalues are $1$ and $0.1$  and $\frac{1^2}{(1^2 + 0.05)^2} = 0.036 >  \frac{0.1^2}{(0.1^2 + 0.05)^2} = 0.0177$ so the variance increases mostly in the direction corresponding to the larger singular values. 

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q1/RR_GD_scatterplot0.pdf}
		\caption{Scatterplot of the gradient-descent estimate \newline corresponding to different noise realizations and iteration step $k=3$.}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q1/RR_GD_scatterplot1.pdf}
		\caption{Scatterplot of the gradient-descent estimate \newline corresponding to different noise realizations and iteration step $k=50$.}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q1/RR_GD_scatterplot2.pdf}
		\caption{Scatterplot of the gradient-descent estimate \newline corresponding to different noise realizations and iteration step $k=500$.}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q1/RR_GD_scatterplot_distribution0.pdf}
		\caption{Heatmap of the distribution of the estimate following a Gaussian distribution with the mean and covariance matrix derived in part c, iteration step $k=3$.}
	\end{figure}

 
 
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q1/RR_GD_scatterplot_distribution1.pdf}
		\caption{Heatmap of the distribution of the estimate following a Gaussian distribution with the mean and covariance matrix derived in part c, iteration step $k=50$.}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q1/RR_GD_scatterplot_distribution2.pdf}
		\caption{Heatmap of the distribution of the estimate following a Gaussian distribution with the mean and covariance matrix derived in part c, iteration step $k=500$.}
	\end{figure}

\end{enumerate}
 
\newpage

\item  (Climate modeling) In this problem we model temperature trends using a linear regression model. The file 
\texttt{t\_data.csv} contains the maximum temperature measured
  each month in Oxford from 1853-2014.  We will use the first
  150 years of data (the first $150\cdot 12$ data points) as a training set, and
  the remaining 12 years as a test set.

 In order to fit the evolution of the temperature over the years, we fit the following model
  \begin{align}
  y[t] = a + bt + c \cos(2\pi t/T) + d\sin(2\pi  t/T)
  \end{align}
  where $a,b,c,d\in\R$, $y[t]$ denotes the maximum temperature in Celsius during month $t$ of the dataset (with $t$ starting from $0$ and ending at $162\cdot 12-1$).
   
  \begin{enumerate}
  \item What is the number of parameters in your model and how many data points do you have to fit the model? Are you worried about overfitting?\\
  The number of parameters if $4$ much less than the number of training samples $150\cdot 12 = 1800$, so we have enough samples compared to the number of parameters and our linear regression model will not overfit as there are enough data points.
  Since we want to model the trend of the temperature over the years we have enough datapoints to detect the trend of the  temperature. If we did not have enough points compared to the number of parameters our model will overfit, fitting mostly the noise.
  
  \item Fit the model using least squares on the training set to
    find the coefficients for values of $T$ equal to 1,2,\ldots,20. Which of these models provides a better fit? Explain why this is the case. In the remaining question we will fix $T$ to the value $T^{\ast}$ that provides a better fit.\\ \\

    When we plot the training errors we find out the model with the better fit is for $T=12$.  If we look at the different estimated coefficients, the cosine term is the larger compared to the sine term:
      
    \begin{center}
    		\begin{tabular}{ | c | c | c | c | }
   		\hline
			13.407 & 0 &  -7.62 & -0.484  \\ 
		\hline
    	\end{tabular}
    \end{center}
    
    The large absolute value on the cosine term implies that the phase of the cosine sinusoidal captures most of the cycle of the temperature oscillations.
    $T=12$ corresponds also to the number of months in a year and is the period to which the temperatures fall in  
    the same range from one year to the other for the corresponding month.
    
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q2/train_errors.pdf}
		\caption{Training errors of the model using $T$ equal to 1,2,\ldots,20.}
	\end{figure}
	

    
  \item Produce two plots comparing the actual maximum temperatures with
    the ones predicted by your model for $T:=T^{\ast}$; one for the training set and one for the test set. \\
    
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q2/train_predictions_12_fit.pdf}
		\caption{Actual and predicted max. temperatures from 1853-2014 \newline for $T=12$ on the training set.}
		\label{complexmodel}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q2/test_predictions_12_fit.pdf}
		\caption{Actual and predicted max. temperatures from 1853-2014 \newline for $T=12$ on the test set.}
	\end{figure}
    
 \item Fit the modified model  
   \begin{align}
  y[t] = a + bt + d \sin(2\pi t/T^{\ast})
  \end{align}
and plot the fit to the training data as in the previous question. Explain why it is better to also include a cosine term in the model.\\ \\

Using the $T=12$ the model estimate coefficients are about the same as the ones with the model in part b but without the cosine term:
    \begin{center}
    		\begin{tabular}{ | c | c | c | c | }
   		\hline
			13.394 & 0 & -0.484  \\ 
		\hline
    	\end{tabular}
    \end{center}
    
Not having the cosine term simplify the model but we are no longer capturing all the frequencies of the fundamental seasonal frequency of the temperatures. 
And this was confirmed by part b where, the estimate coefficient corresponding to the cosine term was much larger in comparison to the sine term.
 It is better to combine sine and cosine terms to span more frequencies. 
 If we look at the prediction vs. actual, the predictions follow an almost flat trend since the oscillations corresponding to the sine term are very small being scaled by a small estimated coefficient.

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q2/simplified_model_train_predictions_12_fit.pdf}
		\caption{Actual and predicted max. temperatures from 1853-2014 \newline for $T^{\ast}=12$ on the training set, using the model $y[t] = a + bt + d \sin(2\pi t/T^{\ast})$.}
		\label{simplemodel}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{code/Q2/simplified_model_test_predictions_12_fit.pdf}
		\caption{Actual and predicted max. temperatures from 1853-2014 \newline for $T^{\ast}=12$ on the test set, using the model $y[t] = a + bt + d \sin(2\pi t/T^{\ast})$.}
	\end{figure}


   \item Provide an intuitive interpretation of the coefficients $a$, $b$, $c$ and $d$, and the corresponding features. According to your model, are temperatures rising in Oxford? By how much?\\
   \begin{itemize}
   	\item a is the intercept or expected mean of the temperatures over the years
	\item b corresponds to the trend or drift of the temperatures
	\item c and d are related to the seasonal frequencies of the temperatures, they are included to capture the vertical or horizontal shift of these temperatures over time and their periodicity
   \end{itemize}
   If we look at the trend of the predicted temperatures on the training set (see fig. \ref{complexmodel}) 
   or if we use a value of $T$ which corresponds to a model less sensitive to the seasonality of the temperatures, like the trend
   exhibited by the model in part d (see fig. \ref{simplemodel}), we observe that the temperatures
   have been increasing in Oxford by 2-3 degrees for the last $100$ years (exactly by $1.75$ degree Celsius using the last $100$ years of data).
   
  \end{enumerate}
   
\newpage
   
 \item (Sines and cosines)
  Let $x:[-1/2,1/2)\to\R $ be a real-valued square-integrable function defined on the interval $[-1/2,1/2)$, i.e. $x\in L_2[-1/2,1/2)$. The Fourier series coefficients of $x$, are given by
\begin{align}
\hat{x}[k] & :=  \PROD{x}{\phi_k} = \int_{-1/2}^{1/2} x(t) \exp \brac{- i2 \pi k t}  \diff{t}, \quad k \in \Z,
\end{align}
and the corresponding Fourier series of order $k_c$ equals
\begin{align}
\ml{F}_{k_c}\{x\}(t) = \sum_{k=-k_c}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t}.
\end{align}
As we will discuss in class, this is a representation of $x$ in a basis of complex exponentials. In this problem we show that for real signals the Fourier series is equivalent to a representation in terms of cosine and sine functions.
  \begin{enumerate}
  \item Prove that $\hat{x}[k]=\overline{\hat{x}[-k]}$
    for all $k\in\Z$. [Hint: What is $\overline{e^{it}}$?]\\
    \begin{align*}
    	\overline{\hat{x}[-k]} 	&= 	\overline{  \int_{-1/2}^{1/2} x(t) \exp \brac{ i 2 \pi k t}  \diff{t} } \\
					&=	 \int_{-1/2}^{1/2} \overline{x(t)} \overline{ \exp \brac{ i2 \pi k t} } \diff{t} \\
					&=	 \int_{-1/2}^{1/2} x(t)  \exp \brac{ -i 2 \pi k t} \diff{t} \\
					&= 	 \hat{x}[k] \\
    \end{align*}
    
    
  \item Show that the Fourier series of $x$ of order $k_c$ can be written as
    $$\ml{F}_{k_c}\{x\}(t) = a_0 + \sum_{k=1}^{k_c} a_k\cos(2\pi
    kt)+b_k\sin(2\pi kt),$$
    for some $a_0,\ldots,a_k,b_1,\ldots,b_k\in\R$. [Hint: Group terms
      in $\ml{F}_{k_c}\{x\}(t)$ corresponding to $\pm k$ and use previous
      part.  What is the real part of $zw$ for $z,w\in\C $?]
      
    \begin{align*}
		\ml{F}_{k_c}\{x\}(t) 	&=	\sum_{k=-k_c}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t} \\
						&= 	\hat{x}[0] +  \sum_{k=-k_c}^{-1} \hat{x}[k] \exp \brac{i 2 \pi k t} + \sum_{k=1}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t} \\
						&= 	\hat{x}[0] +  \sum_{k=k_c}^{1} \hat{x}[-k] \exp \brac{- i 2 \pi k t} + \sum_{k=1}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t} \\
						&=	\hat{x}[0] +   \sum_{k=1}^{k_c} (\hat{x}[-k] \exp \brac{- i 2 \pi k t} +  \hat{x}[k] \exp \brac{i 2 \pi k t} ) \\
						&=	\hat{x}[0] +   \sum_{k=1}^{k_c} (\overline{ \hat{x}[k] } \exp \brac{- i 2 \pi k t} +  \hat{x}[k] \exp \brac{i 2 \pi k t} ) \text{ ~ using part a } \\
   \end{align*}
Given two complex numbers $z = a + i b$ and $w = c + id$, we have $zw = ac - bd + i (ad + bc)$ and $\overline{z} \; \overline{w} = ac - bd - i (ad + bc)$ giving that $z w + \overline{z} \overline{w} = 2 (ac - bd)$.
Let  $z_k = \hat{x}[k]$ and $w_k =     \exp \brac{i 2 \pi k t} $ thus
   \begin{align*}
   	\ml{F}_{k_c}\{x\}(t) 		&=  \hat{x}[0] +  \sum_{k=1}^{k_c} 2 (\Real{ \hat{x}[k]} \cos{ 2 \pi k t} - \Imag{\hat{x}[k] }  \sin{ 2 \pi k t} ) \\
						&= \hat{x}[0] +  \sum_{k=1}^{k_c} (2 \Real{ \hat{x}[k]}) \cos{ 2 \pi k t} + (-2  \Imag{\hat{x}[k] } ) \sin{ 2 \pi k t}  \\
						&= \hat{x}[0] +   \sum_{k=1}^{k_c} a_k \cos(2\pi k t) + b_k \sin(2\pi k t) \\
   \end{align*}
   
  \item Give
    expressions for the coefficients $a_k,b_k$ for $k\geq 1$ from the
    previous part as real integrals. Interpret them in terms of inner products.
    
    From the definition 
   \begin{align*}
  				 \hat{x}[k] 	&= \PROD{x}{\phi_k} = \int_{-1/2}^{1/2} x(t) \exp \brac{- i2 \pi k t}  \diff{t}   \text{ for } k\geq 1\\
				 		&=  	\int_{-1/2}^{1/2} x(t) (\cos(2\pi k t) + i \sin(2\pi k t)) \diff{t} \\
						&=	 \int_{-1/2}^{1/2} x(t)  \cos(2\pi k t) \diff{t} + i \int_{-1/2}^{1/2} x(t) \sin(2\pi k t) \diff{t} \\
						&=	\Real{ \hat{x}[k]} + i   \Imag{\hat{x}[k] } \\			
   \end{align*}
   thus 
   $$a_k =2 \int_{-1/2}^{1/2} x(t)  \cos(2\pi k t) \diff{t} = 2  \PROD{x}{ \cos(2\pi k t) } =  2  \PROD{x}{\Real{ \phi_k}} $$ 
   and $$b_k =  2 \int_{-1/2}^{1/2} x(t)   \sin(2\pi k t) \diff{t} = 2  \PROD{x}{ \sin(2\pi k t) } =  2  \PROD{x}{\Imag{ \phi_k}}$$.
          
  \item Suppose $x(t)=\cos(2\pi(t+\phi))$ for some fixed
    $\phi\in\R$.  What are the Fourier coefficients of $x$?\\
    We can express the sinusoid $x(t)$ as:
  \begin{align*}   
  	\cos(2\pi(t+\phi) 	&= \frac{1}{2} [ e^{i 2 \pi (t+\phi)}  + e^{-i 2 \pi (t+\phi)} ] \\
					&=  \frac{e^{-i 2 \pi \phi}}{2} e^{-i 2 \pi t}  + \frac{e^{i 2 \pi \phi}}{2} e^{i 2 \pi t} \\
   \end{align*}
   So the Fourier coefficients of $x$ are $\hat{x}[-1] = \frac{e^{-i 2 \pi \phi}}{2}$ and $\hat{x}[1] = \frac{e^{i 2 \pi \phi}}{2}$.
       
  \item Suppose that $f$ is also even (i.e., $x(-t)=x(t)$).  Prove
    that the Fourier coefficients are all real (i.e., that
    $\hat{x}[k]\in\R$ for all $k\in\Z$).
    
    Using part a
      \begin{align*}
      	\overline{\hat{x}[k]}	&= 	\hat{x}[-k] \\
					&= 	\int_{-1/2}^{1/2} x(t) \exp \brac{i 2 \pi k t}  \diff{t} \\
					&= 	\int_{1/2}^{-1/2} x(-u) \exp \brac{- i 2 \pi k u}  (- \diff{u})  \text{ by change of variable } u = -t \\
					&=	\int_{-1/2}^{1/2} x(t) \exp \brac{- i 2 \pi k t}  \diff{t} \text{ ~ since x is even }\\
					&=	\hat{x}[k] \\
       \end{align*}  
       $\overline{\hat{x}[k]} = \hat{x}[k]$, $\hat{x}[k]  \in \R$ for all $k\in\Z$.
       
  \end{enumerate} 
   
 \end{enumerate}
\end{document}
