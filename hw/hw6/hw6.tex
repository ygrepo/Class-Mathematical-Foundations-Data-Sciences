\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}

\input{macros}

\begin{document}

\begin{center}
{\large{\textbf{Homework 6}} } \vspace{0.2cm}\\
Due April 5 at 11 pm
\end{center}
Yves Greatti - yg390\\

\begin{enumerate}

\item (Gradient descent and ridge regression) In this problem we study the iterations of gradient descent applied to the ridge-regression cost function
\begin{align}
\min_{\beta \in \R^p} \frac{1}{2}\normTwo{y-X^T\beta}^2 + \frac{\lambda}{2}\normTwo{\beta}^2,
\end{align}
where $X \in \R^{p \times n}$ is a fixed feature matrix and $y\in \R^{n}$ is a response vector. (The factor of 1/2 is just there to make calculations a bit cleaner.)
\begin{enumerate}
\item Derive a closed form expression for the value of the estimated coefficient $\beta^{\brac{k}}$ at the $k$th iteration of gradient descent initialized at the origin in terms of the SVD of $X$ when the step size is constant. \\

The gradient-descent updates are:
\begin{align*}
		\beta^{(k+1)}	&=	\beta^{(k)} - \; \alpha_k \nabla_{\beta} f(\beta^{(k)})	\\
					&=	\beta^{(k)}  - \alpha_k (X X^T \beta^{(k)} - X y +  \lambda \beta^{(k)}) \\
					&=    ((1- \lambda \alpha_k) I  - \alpha_k X X^T) \beta^{(k)}  +  \alpha_k  X y  \\
					&=	\big( ( 1 - \alpha \lambda) I - \alpha X X^T  \big)^{k+1} \beta^{(0)}  +	\alpha \sum_{i=0}^k \big( (1 - \alpha \lambda) I - \alpha X X^T \big)^i X y \\
					&=	\alpha \sum_{i=0}^k \big( (1 - \alpha \lambda) I - \alpha X X^T \big)^i X y \
\end{align*}
since the step size $\alpha_k = \alpha$ is constant and $\beta^{(0)}$ is the zero vector (initialization at the origin).    
Let the svd of $X = U S V^T$ then 
\begin{align*}
		\beta^{(k+1)}	&= \alpha  \sum_{i=0}^k \big( (1 - \alpha \lambda) I - \alpha U S^2 U^T  \big)^i U S V^T y \\
\end{align*}

Assuming $p \le n$ and $X$ is full rank, $UU^T = U^T U = I$ and we have:
\begin{align*}
	\beta^{(k+1)}	&=  \alpha  \sum_{i=0}^k \big( (1  - \alpha \lambda) UU^T- \alpha U S^2 U^T \big)^i U S V^T y \\
				&= \alpha U   \sum_{i=0}^k \big( ( 1 - \alpha \lambda) I - \alpha S^2  \big)^i  S V^T y \\
				&=  \alpha U   \text{diag}_{j=1}^p \sum_{i=0}^k \big(  1 - \alpha (s_j^2 + \lambda)  \big)^i  S V^T y \\
				&=  U   \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k+1} s_j}{s_j^2 + \lambda}  \;  V^T y \\ 
\end{align*}

\item Under what condition on the step size does gradient descent converge to the ridge-regression coefficient estimate as $k \rightarrow \infty$?\\
If step size $\alpha$ is small enough: $0 < \alpha < \frac{2} {\lambda + s_1^2} \le \frac{2} {\lambda + s_j^2} \rightarrow |1 - \alpha (s_j^2 + \lambda)| < 1$ then
 $\lim_{k \rightarrow \infty} (1 - \alpha (s_j^2 + \lambda))^k =0, j=1, \dots ,p $, gradient descent converges to:
\begin{align*}
	\lim_{k \rightarrow \infty} \beta^{(k)}	&= U  \;  \text{diag}_{j=1}^p \bigg( \frac{s_j} {s_j^2 + \lambda} \bigg)  V^T y \\
									&= U (S^2 + \lambda I)^{-1} S^2 V^T y
\end{align*}
which are the ridge-regression coefficient estimates.

\item Assume the following additive model for the data:  
\begin{align}
\ry_{\op{train}} := X^T \beta_{\op{true}} + \rnd{z}_{\op{train}},
\end{align}
where $\rnd{z}_{\op{train}}$ is modeled as an $n$-dimensional iid Gaussian vector with zero mean and variance $\sigma^2$. What is the distribution of the estimated coefficient $\rnd{\beta}^{\brac{k}}$ at the $k$th iteration of gradient descent initialized at the origin?\\
Using the expression of the estimated coefficients from part a, we now have:

\begin{align*}
	\rnd{\beta}^{\brac{k}} &=  U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j}{s_j^2 + \lambda}  \;  V^T (X^T \beta_{\op{true}} + \rnd{z}_{\op{train}} )\\
					&=  U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j}{s_j^2 + \lambda}  \;  V^T (V S U^T \beta_{\op{true}} + \rnd{z}_{\op{train}} )\\
					&=  U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j^2}{s_j^2 + \lambda}  \; U^T \beta_{\op{true}} 
					+      U  \text{diag}_{j=1}^p \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j}{s_j^2 + \lambda}  \;  V^T  \rnd{z}_{\op{train}} \\
\end{align*}
Using theorem 8,6 from the notes on PCA, then  the estimated coefficient $\rnd{\beta}^{\brac{k}}$ at the $k$th iteration of gradient descent initialized at the origin have is a Gaussian random vector  with mean:

$$ \beta_{\text{GD}}  = \sum_{j=1}^p  \frac{ 1 - ( 1 - \alpha (s_j^2 + \lambda))^{k} s_j^2}{s_j^2 + \lambda}  \PROD{u_j}{\beta_{\op{true}}} u_j $$
and covariance matrix

$$ \Sigma_{\text{GD}} = \sigma^2 U  \text{diag}_{j=1}^p \frac{ (1 - ( 1 - \alpha (s_j^2 + \lambda))^{k})^2 s_j^2}{(s_j^2 + \lambda)^2}  U^T $$

\item Complete the script \emph{RR\_GD\_landscape.py} in order to verify your answer to the previous question. Report the figures generated by the script.
\end{enumerate}
 
\newpage

\item  (Climate modeling) In this problem we model temperature trends using a linear regression model. The file 
\texttt{t\_data.csv} contains the maximum temperature measured
  each month in Oxford from 1853-2014.  We will use the first
  150 years of data (the first $150\cdot 12$ data points) as a training set, and
  the remaining 12 years as a test set.

 In order to fit the evolution of the temperature over the years, we fit the following model
  \begin{align}
  y[t] = a + bt + c \cos(2\pi t/T) + d\sin(2\pi  t/T)
  \end{align}
  where $a,b,c,d\in\R$, $y[t]$ denotes the maximum temperature in Celsius during month $t$ of the dataset (with $t$ starting from $0$ and ending at $162\cdot 12-1$).
   
  \begin{enumerate}
  \item What is the number of parameters in your model and how many data points do you have to fit the model? Are you worried about overfitting?
  \item Fit the model using least squares on the training set to
    find the coefficients for values of $T$ equal to 1,2,\ldots,20. Which of these models provides a better fit? Explain why this is the case. In the remaining question we will fix $T$ to the value $T^{\ast}$ that provides a better fit.
  \item Produce two plots comparing the actual maximum temperatures with
    the ones predicted by your model for $T:=T^{\ast}$; one for the training set and one for the test set. 
 \item Fit the modified model  
   \begin{align}
  y[t] = a + bt + d \sin(2\pi t/T^{\ast})
  \end{align}
and plot the fit to the training data as in the previous question. Explain why it is better to also include a cosine term in the model.
   \item Provide an intuitive interpretation of the coefficients $a$, $b$, $c$ and $d$, and the corresponding features. According to your model, are temperatures rising in Oxford? By how much?
  \end{enumerate}
   
 \item (Sines and cosines)
  Let $x:[-1/2,1/2)\to\R $ be a real-valued square-integrable function defined on the interval $[-1/2,1/2)$, i.e. $x\in L_2[-1/2,1/2)$. The Fourier series coefficients of $x$, are given by
\begin{align}
\hat{x}[k] & :=  \PROD{x}{\phi_k} = \int_{-1/2}^{1/2} x(t) \exp \brac{- i2 \pi k t}  \diff{t}, \quad k \in \Z,
\end{align}
and the corresponding Fourier series of order $k_c$ equals
\begin{align}
\ml{F}_{k_c}\{x\}(t) = \sum_{k=-k_c}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t}.
\end{align}
As we will discuss in class, this is a representation of $x$ in a basis of complex exponentials. In this problem we show that for real signals the Fourier series is equivalent to a representation in terms of cosine and sine functions.
  \begin{enumerate}
  \item Prove that $\hat{x}[k]=\overline{\hat{x}[-k]}$
    for all $k\in\Z$. [Hint: What is $\overline{e^{it}}$?]\\
    \begin{align*}
    	\overline{\hat{x}[-k]} 	&= 	\overline{  \int_{-1/2}^{1/2} x(t) \exp \brac{ i 2 \pi k t}  \diff{t} } \\
					&=	 \int_{-1/2}^{1/2} \overline{x(t)} \overline{ \exp \brac{ i2 \pi k t} } \diff{t} \\
					&=	 \int_{-1/2}^{1/2} x(t)  \exp \brac{ -i 2 \pi k t} \diff{t} \\
					&= 	 \hat{x}[k] \\
    \end{align*}
    
    
  \item Show that the Fourier series of $x$ of order $k_c$ can be written as
    $$\ml{F}_{k_c}\{x\}(t) = a_0 + \sum_{k=1}^{k_c} a_k\cos(2\pi
    kt)+b_k\sin(2\pi kt),$$
    for some $a_0,\ldots,a_k,b_1,\ldots,b_k\in\R$. [Hint: Group terms
      in $\ml{F}_{k_c}\{x\}(t)$ corresponding to $\pm k$ and use previous
      part.  What is the real part of $zw$ for $z,w\in\C $?]
      
    \begin{align*}
		\ml{F}_{k_c}\{x\}(t) 	&=	\sum_{k=-k_c}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t} \\
						&= 	\hat{x}[0] +  \sum_{k=-k_c}^{-1} \hat{x}[k] \exp \brac{i 2 \pi k t} + \sum_{k=1}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t} \\
						&= 	\hat{x}[0] +  \sum_{k=k_c}^{1} \hat{x}[-k] \exp \brac{- i 2 \pi k t} + \sum_{k=1}^{k_c} \hat{x}[k] \exp \brac{i 2 \pi k t} \\
						&=	\hat{x}[0] +   \sum_{k=1}^{k_c} (\hat{x}[-k] \exp \brac{- i 2 \pi k t} +  \hat{x}[k] \exp \brac{i 2 \pi k t} ) \\
						&=	\hat{x}[0] +   \sum_{k=1}^{k_c} (\overline{ \hat{x}[k] } \exp \brac{- i 2 \pi k t} +  \hat{x}[k] \exp \brac{i 2 \pi k t} ) \text{ ~ using part a } \\
   \end{align*}
Given two complex numbers $z = a + i b$ and $w = c + id$, we have $zw = ac - bd + i (ad + bc)$ and $\overline{z} \; \overline{w} = ac - bd - i (ad + bc)$ giving that $z w + \overline{z} \overline{w} = 2 (ac - bd)$.
Let  $z_k = \hat{x}[k]$ and $w_k =     \exp \brac{i 2 \pi k t} $ thus
   \begin{align*}
   	\ml{F}_{k_c}\{x\}(t) 		&=  \hat{x}[0] +  \sum_{k=1}^{k_c} 2 (\Real{ \hat{x}[k]} \cos{ 2 \pi k t} - \Imag{\hat{x}[k] }  \sin{ 2 \pi k t} ) \\
						&= \hat{x}[0] +  \sum_{k=1}^{k_c} (2 \Real{ \hat{x}[k]}) \cos{ 2 \pi k t} + (-2  \Imag{\hat{x}[k] } ) \sin{ 2 \pi k t}  \\
						&= \hat{x}[0] +   \sum_{k=1}^{k_c} a_k \cos(2\pi k t) + b_k \sin(2\pi k t) \\
   \end{align*}
   
  \item Give
    expressions for the coefficients $a_k,b_k$ for $k\geq 1$ from the
    previous part as real integrals. Interpret them in terms of inner products.
    
    From the definition 
   \begin{align*}
  				 \hat{x}[k] 	&= \PROD{x}{\phi_k} = \int_{-1/2}^{1/2} x(t) \exp \brac{- i2 \pi k t}  \diff{t}   \text{ for } k\geq 1\\
				 		&=  	\int_{-1/2}^{1/2} x(t) (\cos(2\pi k t) + i \sin(2\pi k t)) \diff{t} \\
						&=	 \int_{-1/2}^{1/2} x(t)  \cos(2\pi k t) \diff{t} + i \int_{-1/2}^{1/2} x(t) \sin(2\pi k t) \diff{t} \\
						&=	\Real{ \hat{x}[k]} + i   \Imag{\hat{x}[k] } \\			
   \end{align*}
   thus 
   $$a_k =2 \int_{-1/2}^{1/2} x(t)  \cos(2\pi k t) \diff{t} = 2  \PROD{x}{ \cos(2\pi k t) } =  2  \PROD{x}{\Real{ \phi_k}} $$ 
   and $$b_k =  2 \int_{-1/2}^{1/2} x(t)   \sin(2\pi k t) \diff{t} = 2  \PROD{x}{ \sin(2\pi k t) } =  2  \PROD{x}{\Imag{ \phi_k}}$$.
          
  \item Suppose $x(t)=\cos(2\pi(t+\phi))$ for some fixed
    $\phi\in\R$.  What are the Fourier coefficients of $x$?
  \item Suppose that $f$ is also even (i.e., $x(-t)=x(t)$).  Prove
    that the Fourier coefficients are all real (i.e., that
    $\hat{x}[k]\in\R$ for all $k\in\Z$).
    
    Using part a
      \begin{align*}
      	\overline{\hat{x}[k]}	&= 	\hat{x}[-k] \\
					&= 	\int_{-1/2}^{1/2} x(t) \exp \brac{i 2 \pi k t}  \diff{t} \\
					&= 	\int_{1/2}^{-1/2} x(-u) \exp \brac{- i 2 \pi k u}  (- \diff{u})  \text{ by change of variable } u = -t \\
					&=	\int_{-1/2}^{1/2} x(t) \exp \brac{- i 2 \pi k t}  \diff{t} \text{ ~ since x is even }\\
					&=	\hat{x}[k] \\
       \end{align*}  
       $\overline{\hat{x}[k]} = \hat{x}[k]$, $\hat{x}[k]  \in \R$ for all $k\in\Z$.
       
  \end{enumerate} 
   
 \end{enumerate}
\end{document}
