\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}


\input{macros}

\begin{document}

\begin{center}
{\large{\textbf{Homework 9}} } \vspace{0.2cm}\\
Due April 26 at 11 pm
\end{center}
Yves Greatti - yg390\\


\begin{enumerate}

\item (Real discrete sinusoids)

 \begin{enumerate}
 \item Prove that for any $x,y\in\R$
  \begin{align}
 \cos(x+y) & = \cos(x)\cos(y) - \sin(x)\sin(y) ,\\
 \sin(x+y) & = \cos(x)\sin(y) + \sin(x)\cos(y) .
 \end{align}\\
 
 \begin{align*}
	  \exp(i (x+y)) 					&= \exp(ix) \exp(iy) \\
 	 \cos(x+y) + i \sin(x+y)			&= (\cos(x) + i \sin(x))(\cos(y) + i \sin(y)) \\
	  \cos(x+y) + i \sin(x+y)			&= (\cos(x) \cos(y) -  \sin(x) \sin(y)) + i (\cos(x) \sin(y) +  \sin(x) \cos(y)) \\
 \end{align*}
Equality of the real part of LHS to the real part of RHS gives the first equality and similarly equality of  the imaginary parts gives the second equality.
 
 \item Use the result from part (a) to show that the real discrete sinusoidal vectors
 \begin{align}
 c_0 & = \frac{1}{\sqrt{N}}\MAT{1 \\ 1 \\ \cdots \\ 1},\\
c_k & = \sqrt{\frac{2}{N}}\MAT{1 \\ \cos \brac{\frac{2 \pi k }{N}} \\ \cdots \\ \cos \brac{\frac{2 \pi k (N-1))}{N}}}, \quad 1 \leq k \leq \frac{N-1}{2},\\
 s_k & = \sqrt{\frac{2}{N}}\MAT{0 \\ \sin \brac{\frac{2 \pi k }{N}} \\ \cdots \\ \sin \brac{\frac{2 \pi k (N-1))}{N}}}, \quad 1 \leq k \leq \frac{N-1}{2},
 \end{align}
 where we assume that $N$ is odd, form an orthonormal basis of $\R^{N}$.\\
Let $x=\frac{2 k \pi}{N}, \|c_0||^2 =  \frac{1}{\sqrt{N}} \sqrt{\sum_{j=1}^N 1} = \frac{\sqrt{N}} {\sqrt{N}} = 1$ 
and $\|c_k\|^2 = \frac{2}{N} \sum_{j=0}^{N-1} \cos(j x)^2, \|s_k\|^2 = \frac{2}{N} \sum_{j=1}^{N-1} \sin(j x)^2)$
 
 \begin{align*}
 \text{from (a)}	\cos(2 x)			&= \cos(x)^2 - \sin(x)^2 \\
			\cos(x)^2 			&= \frac{1 + \cos(2 x)} {2} \\
			\sin(x)^2			&=  \frac{1 - \cos(2 x)} {2} \\
			\sin(x+y) 			& = \cos(x)\sin(y) + \sin(x)\cos(y)\\
			\sin(x - y)			&= -\cos(x) \sin(y) + \sin(x) \cos(y) \\
			 2 \cos(x) \sin(y)	&= \sin(x + y) - \sin( x- y) \\
 \end{align*}
 $N$ is odd so $\sin(\frac{x}{2}) = \sin( \frac{2 k \pi}{2N} ) = \sin( \frac{k \pi}{N} ) \neq 0$ thus and using the last equality and telescoping, we have 
 \begin{align*}
 		2 \sin(\frac{x}{2}) \sum_{k=1}^N \cos(k x)	&=  \sum_{k=1}^N  2 \sin(\frac{x}{2})  \cos(k x) \\
										&= \sum_{k=1}^N \bigg( \sin \bigg( (k + \frac{1}{2}) x \bigg) -  \sin \bigg( (k - \frac{1}{2}) x \bigg) \bigg) \\
										&= \sin \bigg( (N + \frac{1}{2}) x \bigg) - \sin(\frac{x}{2})\\
 \end{align*}
 \begin{align*}
	  \sum_{k'=1}^{N-1} \cos(k' x)	&= \frac{ \sin \bigg( (N - \frac{1}{2}) \frac{2 k \pi}{N} \bigg)   - \sin(\frac{k \pi}{N}) } {2 \sin( \frac{k \pi}{N} )} \\
	  						&= \frac{-2 \sin( \frac{k \pi}{N} ) } {2 \sin( \frac{k \pi}{N} )} \\
							&= -1 \\
\end{align*}
 Similarly we find that
  \begin{align*}
   	\sum_{k'=1}^{N-1} \sin(k' x)	&=  \frac{ \cos(\frac{k \pi}{N}) - \cos \bigg( (N - \frac{1}{2}) \frac{2 k \pi}{N} \bigg) } {2 \sin( \frac{k \pi}{N} )} \\
							&= 0
 \end{align*}
 
 \ \begin{align*}			
	  \sum_{k'=1}^{N-1} \cos(k' x)^2	&=    \sum_{k'=1}^{N-1}  \frac{1 + \cos(k' 2 x)} {2} \\
	  						&=	\frac{N-1}{2} + \frac{1}{2}  \sum_{k'=1}^{N-1}  \cos(k' 2 x) \\
							&=  	\frac{N-2}{2} \\
	\|c_k\|^2 					&= 	 \frac{2}{N} (1 +  \sum_{k'=1}^{N-1} \cos(k' x)^2) \\
							&=  	\frac{2}{N} \frac{N}{2} = 1 \\	
	\|s_k\|^2 					&=.  	\frac{2}{N} \sum_{k'=1}^{N-1} \sin (k' x)^2 \\
	 \sum_{k'=1}^{N-1} \sin (k' x)^2  &= 	 \sum_{k'=1}^{N-1}  \frac{1 - \cos(k' 2 x)} {2} \\	
	 						&=	 \frac{N-1}{2} -\frac{1}{2}  \sum_{k'=1}^{N-1}  \cos(k' 2 x) \\
							&= 	 \frac{N-1}{2} -\frac{1}{2} (-1) \\
							&= 	\frac{N}{2} \\
	\|s_k\|^2 					&= 	\frac{2}{N} \frac{N}{2}  = 1\\											
 \end{align*}
For $1 \leq i,j \leq \frac{N-1}{2}, i \neq j$
 \begin{align*}
 \PROD{c_i}{s_j}		&=	 \sqrt{\frac{4}{N^2}}  \sum_{k=1}^{N-1} \cos \brac{\frac{2 \pi i k }{N}} \sin \brac{\frac{2 \pi j k }{N}} \\
 					&=	 \sqrt{\frac{4}{N^2}}  ( \sum_{k=1}^{N-1}  \sin \brac{\frac{2 \pi (i+j) k }{N}} - \sum_{k=1}^{N-1}  \sin \brac{\frac{2 \pi (i-j) k }{N}}  )\\		
					&= 	 \sqrt{\frac{4}{N^2}}  ( \sum_{k=1}^{N-1}  \sin \brac{\frac{2 \pi \tau k }{N}} - \sum_{k=1}^{N-1}  \sin \brac{\frac{2 \pi \zeta k }{N}}  )\\		
					&= 0 \text{ using the result few lines above about the sum of sines}
  \end{align*}
   \end{enumerate}
 
 \newpage
 \item (PCA of stationary vector) Let $\rv{x}$ be a wide-sense stationary vector with real-valued autocovariance vector $a_{\rx}$,  with covariance matrix $\Sigma_{\rv{x}}$ . In the notes we showed that the eigenvectors and eigenvalues of $\Sigma_{\rv{x}}$ are complex exponentials and the DFT coefficients of $a_{\rx}$ respectively. Here we will show that we can derive an equivalent real-valued eigendecomposition because the autocovariance vector is real. We will assume that $N$ is an odd number. (Hint: You will find the results from Problem 1 useful.)
 \begin{enumerate}
 \item Show that the DFT coefficients of $a_{\rx}$ are real, and satisfy $\hat{a}_{\rx}[k] = \hat{a}_{\rx}[N-k]$ for $k=1,\ldots,\frac{N-1}{2}$.\\
 From the notes on stationarity, the covariance matrix $\Sigma_{\rv{x}}$ is a symmetric circulant matrix which has for rows the real-valued autocovariance vector $a_{\rx}$ and shifted up to $N-1$ autocovariance vectors.
 Being a real symmetric matrix, eigenvalues of $\Sigma_{\rv{x}}$  are real. And (from Theorem 4.3), the eigendecomposition of the  covariance matrix $\Sigma_{\rv{x}}$ has for eigenvalues the DFT coefficients  of $a_{\rx}$:
 $$\Sigma_{\rv{x}} = \frac{1}{N} F_{[N]}^* \text{diag}(\hat{a}_{\rx}) F_{[N]}$$ which implies that  $\hat{a}_{\rx}$ are real.
 For the proof of a symmetric matrix has all its eigenvalues real, we have, let $A=A^*$ and $v\neq 0$ being an eigenvector with eigenvalue $\lambda$:
 \begin{align*}
 	\PROD{v}{Av}	&= \PROD{v}{\lambda v}  = \overline{\lambda} \PROD{v}{v}  \\
 	\PROD{Av}{v}	&= \PROD{\lambda v}{v}  = \lambda \PROD{v}{v}  \\
	\lambda		&=  \overline{\lambda}
 \end{align*}
 
 \item Show that
 \begin{align}
 \Sigma_{\rv{x}}[j_1,j_2] & = \frac{1}{N}\sum_{k=0}^{N-1} \hat{a}_{\rx}[k] \exp\brac{\frac{i2 \pi k (j_2-j_1)}{N}}.
 \end{align}  \\
 With $\psi_{k}[j] = \exp( \frac{i 2 \pi k j}{N} )$, we have 
 $$F_{[N]} = \MAT{\overline{\psi_0} \\ \overline{\psi_1} \\ \vdots \\ \overline{\psi_{N-1}}}$$
 $$ F_{[N]}^* = \MAT{\psi_0 & \psi_1 & \cdots & \psi_{N-1}}$$
 Thus
 $$\Sigma_{\rv{x}} =  \frac{1}{N} F_{[N]}^* \text{diag}(\hat{a}_{\rx}) F_{[N]} =  \frac{1}{N}  \sum_{k=0}^{N-1} \hat{a}_{\rx}[k] \psi_{k}  \overline{\psi_k}$$
 And $$\Sigma_{\rv{x}}[j_1,j_2] = \frac{1}{N}\sum_{k=0}^{N-1} \hat{a}_{\rx}[k] \exp\brac{\frac{i2 \pi k (j_2-j_1)}{N}}$$
 
 
  \item Derive the real-valued eigenvalues and the corresponding eigenvectors of $\Sigma_{\rv{x}}$.\\
  From the notes on stationarity, $C$ being a circulant matrix and let $x \neq 0$, an eigenvector of $C$, and the previous question, we have
  \begin{align*}
  					y	&= C x =	c * x \\
					y[k]	&= \frac{1}{N}\sum_{k=0}^{N-1} \hat{a}_{\rx}[k] \exp\brac{\frac{i2 \pi k (j_2-j_1)}{N}} \\
						&=  \frac{1}{N}\sum_{k=0}^{N-1} \hat{a}_{\rx}[k]  \exp\brac{\frac{-i2 \pi k j_1}{N}}  \exp\brac{\frac{i2 \pi k j_2}{N}} \\
						&= \frac{1}{N}\sum_{k=0}^{N-1} \hat{a}_{\rx}[k] ( \exp\brac{\frac{i2 \pi k j_1}{N}} )  (\exp\brac{\frac{i2 \pi k j_2}{N}})  \text{ since } \rx \text{ is wide-sense stationary}\\
						&= \frac{1}{N}\sum_{k=0}^{N-1} \hat{a}_{\rx}[k] ( \exp\brac{\frac{i2 \pi j}{N}} )^k    ( \exp\brac{\frac{i2 \pi j}{N}} )^k  \text{ multiplication along the same rows} j_1 = j_2\\
						&=  \frac{1}{N}\sum_{k=0}^{N-1} \hat{a}_{\rx}[k] ( \exp\brac{\frac{i2 \pi j}{N}} )^k  \MAT{1 \\  \exp\brac{\frac{i2 \pi j}{N}}  \\  \exp\brac{\frac{i2 \pi j}{N}}^2 \\ \vdots \\  \exp\brac{\frac{i2 \pi j}{N}}^{N-1}}\\
  \end{align*}
  $x= \frac{1}{N} \MAT{1 \\  \exp\brac{\frac{i2 \pi j}{N}}  \\  \exp\brac{\frac{i2 \pi j}{N}}^2 \\ \vdots \\  \exp\brac{\frac{i2 \pi j}{N}}^{N-1}}$ is the eigenvector for the eigenvalue $\sum_{k=0}^{N-1} \hat{a}_{\rx}[k] ( \exp\brac{\frac{i2 \pi j}{N}} )^k $.
  From part (a)  $\hat{a}_{\rx}[k] = \hat{a}_{\rx}[N-k]$ for $k=1,\ldots,\frac{N-1}{2}$ and using the fact that 
  $$
  \exp\brac{\frac{i2 \pi k j}{N}} +  \exp\brac{\frac{i2 \pi (N-k) j}{N}} = 2 \cos (\frac{i2 \pi k j}{N})
  $$
  Each $\lambda_j$ for $0 \le j \le N-1$, eigenvalue $j$ is:
  $$
  	\lambda_j = \hat{a}_{\rx}[0] +  2 \hat{a}_{\rx}[1] \cos (\frac{i2 \pi j}{N})+ \cdots +   2 \hat{a}_{\rx}[\frac{N}{2}] \cos (\frac{i2 \pi \frac{N}{2} j}{N})
  $$
  
  \end{enumerate}

  \newpage
 \item (Discrete filter) Let us index the DFT coefficients of the $N$-dimensional vectors from $-(N-1)/2$ to $(N-1)/2$ (assuming $N$ is odd). We define the bandlimited signals in this space as those for which the nonzero Fourier coefficients are zero beyond a certain value $k_c$, i.e. $x \in \C^{N}$ is bandlimited if $\hat{x}[k]=0$ for all $\abs{k}>k_c$. Let $y$ be the vector with the smallest $\ell_2$ norm such that $x \ast y = x$ for all bandlimited vectors with cut-off frequency $k_c$ (where $k_c$ is a fixed integer smaller than $(N-1)/2$). Derive an explicit expression for the entries of $y$, showing that they are real valued.

 
  \item (Deconvolution) In this problem we will tackle image deblurring, an important problem in image processing. Image blurring is often modeled as convolution of the image of interest with a blur kernel.  The file \texttt{wiener.py} contains code to load all the required data and has snippet of code to plot image and filters in pixel domain and fourier domain. 
  
 \begin{enumerate}
 \item Gaussian blur kernels are a very popular model in the literature.  Outline a method to recover the true image from the blurred image. The variable \texttt{blur} in \texttt{wiener.py} contains the 2D convolution of an image with a Gaussian kernel \texttt{filt}. Write a function \texttt{deconvolution()} to recover the true image from \texttt{blur}. Report the result in both the image domain and the Fourier domain (plot the magnitude of the Fourier coefficients). You can adapt the snippet of code in \texttt{wiener.py} to generate the plots. [Hint: Check if a division by zero will nearly occur, and replace with a small non-zero value.]
 \item Apply your method to the image in the variable \texttt{noisy\_blur}, which contains a blurred image corrupted by some noise.  Report the result in the image and the Fourier domains (plot the magnitude of the Fourier coefficients). Explain what is happening.
  \item Derive the Wiener filter for estimating a zero-mean vector $\ry$ from noisy blurred measurements $\rx = b \ast \ry + \rnd{z}$, where $b$ is a known blur kernel and $\rnd{z}$ is iid Gaussian noise with zero mean and variance $\sigma^2$. 
  \item Write a function \texttt{wiener\_deconvolution()} to recover the image from the noisy data (\texttt{noisy\_blur}). Report the result in the image and the Fourier domains (plot the magnitude of the Fourier coefficients). Compare this method to the result in (b) and explain why it works better. [Hint: you may have to use the variable \texttt{all\_images} and \texttt{s} here]
  \end{enumerate}

 \end{enumerate}
\end{document}
