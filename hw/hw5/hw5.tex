\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}

\input{macros}
%\input{Symbols}

\begin{document}

\noindent DS-GA.1013 Mathematical Tools for Data Science \\
Homework 5\\
Yves Greatti - yg390\\

\begin{enumerate}

\item (Augmented dataset) Ridge regression is equivalent to applying OLS on an expanded dataset that has additional examples. Describe these additional examples in detail. Intuitively, what effect do these additional examples have?\\
\medskip

The ridge regression estimate is defined as $\beta_{\op{RR}} = (X X^T + \lambda I)^{-1} X y$, where $X \in \R^{p \times n}, y \in \R^n, \beta \in \R^p$, which can be reformulated as a modified least-square problem
$$\beta_{\op{RR}} = \argmin_\beta \big\| \begin{bmatrix} y   \\ 0 \end{bmatrix} - \begin{bmatrix} X^T   \\ \sqrt{\lambda} I_{n \times p} \end{bmatrix} \beta \big\|_2^2$$
It is like we have added $p$ vectors to the original $n$ datapoints $[x_1, \ldots, x_n]$. 
These rows are constructed using 0 for the dependent variables and the square root of k or zero for the independent variables.
By doing so, Ridge regression embedded the original problem in $\R^n$ into a larger space $\R^{n+p}$ by moving into $p$ different 
directions with a small amount $\sqrt{\lambda}$ which could decrease any collinearity present in the original data points $X$. 
 As we have seen in the notes of linear regression, when the number of training data is small, 
$\lambda$ neutralizes the large variance of the errors between the true  $\beta$ coefficients and the ridge regression coefficients due to small singular values  of the sample covariance matrix.
 
 \newpage
 
\item  (Correlated features) Consider a regression problem where the response only depends on one feature, but we don't know it, so we incorporate an additional feature into the model that happens to be very correlated with the first feature. More specifically, let $y \in \R^{n}$ be defined by
\begin{align}
y := \beta_{\op{true}} w_1 + z, 
\end{align}
where $\beta_{\op{true}} \in \R$ is the true coefficient, $w_1 \in \R^n$ is the first feature vector, and $z \in \R^{n}$ is additive noise. The second feature vector is given by $w_2\in \R^n$ and can be decomposed into
\begin{align}
w_2 = \alpha w_1 + \sqrt{1-\alpha^2} w_{\perp},
\end{align}  
where $w_{\perp}$ is orthogonal to $w_1$. The vectors $w_1$, $w_2$, $w_{\perp}$ and $z$ all have unit $\ell_2$ norm. In addition, we assume
\begin{align}
w_1^Tz = 0.1, \\
w_{\perp}^Tz = 0.1.
\end{align}
We fit a linear regression model to $y$ using the feature matrix 
  \begin{align}
  X & = \MAT{w_1^T \\ w_2^T}.
  \end{align} 
 \begin{enumerate}
  \item What does the OLS estimator of the coefficients $\beta_{\op{OLS}}$ equal to when $\alpha \rightarrow 1$? Explain what is happening. \\
\emph{Hint:} Use the fact that for any $a$, $b$, $c$, and $d$ such that $ad \neq bc$
\begin{align}
 \MAT{a & b \\ c & d}^{-1} = \frac{1}{ad-bc} \MAT{d & -b \\ -c & a}.
\end{align}\\

\medskip

The OLS estimator of the coefficients of $\beta_{\op{OLS}}$  is given by $\beta_{\op{OLS}} = (X X^T)^{-1} X y$.
Expanding each term, we have:
\begin{align*}
 	(X X^T)				&=	\MAT{w_1^T \\ w_2^T} [w_1 \; w_2]	\\
						&=	\MAT{w_1^T w_1 & w_1^T w_2 \\ w_2^T w_1 & w_2^T w_2}	\\
						&= 	\MAT{\|w_1\|_2^2 & w_1^T w_2 \\ w_2^T w_1  & \|w_2\|_2^2} \\
	w_1^T w_2			&=	w_1^T (\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp}) \\
						&=	\alpha \|w_1\|_2^2 + \sqrt{1-\alpha^2}  w_1^T w_{\perp} \\
						&=	\alpha \text{ ~ since } \|w_1\|_2 = 1 \;, w_1 \perp  w_{\perp} \\
	w_2^T w_2			&= 	(\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp})^T (\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp}) \\
						&= 	\alpha^2 w_1^T w_1 + 2 \alpha \sqrt{1-\alpha^2} w_1^T w_{\perp} + (1-\alpha^2) w_{\perp}^T w_{\perp} \\
						&=	\alpha^2 \| w_1 \|_2^2 +  (1-\alpha^2) \|w_{\perp}\|_2^2 \text{ ~  knowing that }  w_{\perp} \perp w_1 \\
						&=	\alpha^2 +  (1-\alpha^2) = 1 \; \text{by assumptions } \| w_1 \|_2 = \|w_{\perp}\|_2 = 1 \\
	\Rightarrow (X X^T)		&= 	\MAT{1 & \alpha \\ \alpha & 1} \\
	\Rightarrow (X X^T)^{-1}	&=	\frac{1}{1-\alpha^2} \MAT{1 & -\alpha \\ -\alpha  & 1} \\
	X y					&= 	\MAT{w_1^T \\ w_2^T} (\beta_{\op{true}} w_1 + z) \\
						&=	\MAT{\beta_{\op{true}} w_1^T w_1 + w_1^T z \\ \beta_{\op{true}} w_2^T w_1 + w_2^T z } \\
						&=	\MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1}	
\end{align*}
Substituting each of the previous terms back into the expression of $\beta_{\op{OLS}}$, we find that:
\begin{align*}
	\beta_{\op{OLS}}		&=	\frac{1}{1-\alpha^2} \MAT{1 & -\alpha \\ -\alpha  & 1}  \MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1} \\
						&= 	\frac{1}{1-\alpha^2}  \MAT{\beta_{\op{true}} + 0.1 -\alpha^2 \beta_{\op{true}} -0.1 \alpha \\ -\alpha \beta_{\op{true}} -0.1 \alpha + \alpha \beta_{\op{true}}  + 0.1} \\
						&=	\frac{1}{1-\alpha^2}  \MAT{(1-\alpha^2)  \beta_{\op{true}} + 0.1 (1-\alpha) \\ 0.1 (1 - \alpha)} \\
						&=  	\MAT{\beta_{\op{true}} + \frac{0.1} {1 + \alpha} \\ \frac{0.1} {1 + \alpha}}
\end{align*}
When $\alpha \rightarrow 1$ , $\beta_{\op{OLS}} \rightarrow   \MAT{\beta_{\op{true}} + 0.05  \\ 0.05}$.
Depending of the value of $\beta_{\op{true}}$ compared to $0.05$, the OLS estimator could consider only the feature $w_1$ and not the correlated feature $w_2$.
It also  adds a fixed constant $0.05$ to the true $\beta_{\op{true}}$ coefficient which could be significant compared to
 $\beta_{\op{true}}$. If we omit this constant  $0.05$ ,  the OLS estimator is unbiased. 
 Notice also that in this case $XX^T$ is rank 1 and not invertible, and in some cases, the algorithm used to find the OLS estimator may be unable to find a solution.
    
   \item  What does the corresponding estimate of the response $y_{\op{OLS}} := X^T\beta_{\op{OLS}}$ equal to when $\alpha \rightarrow 1$? Is it collinear with the true feature $w_1$ when $\alpha \rightarrow 1$? Explain what is happening.\\
   
  \medskip
  Taking $\alpha \rightarrow 1$, $w_2 \rightarrow w_1$ and we have 
  \begin{align*}
	  y_{\op{OLS}} := X^T\beta_{\op{OLS}}	&= [w_1 \; w_2]  \MAT{\beta_{\op{true}} + 0.05  \\ 0.05} \\
	  								&= [w_1 \; w_1] \MAT{\beta_{\op{true}} + 0.05  \\ 0.05} \\
									&= ( \beta_{\op{true}} + 0.1) w_1
  \end{align*}
  When $\alpha \rightarrow 1$, the correlated feature $w_2 \rightarrow w_1$, and the response variable is collinear with the true feature $w_1$, the OLS estimator estimates $y_{\op{OLS}}$ as a linear scaling of $w_1$ 
  up to a factor of $0.1$ which could lead to an important error depending the magnitude of the linear coefficient  $\beta_{\op{true}}$ compared to 0.1. If it is not the case ($0.1$ could be ignored) 
  then the OLS estimator will fit perfectly the training data.
    
  \item What does the ridge regression estimator of the coefficients $\beta_{\op{RR}}$ equal to when $\alpha \rightarrow 1$ and the regularization parameter $\lambda >0$ is fixed? Describe the difference with the OLS estimate.\\
 
  \medskip
  By definition the ridge regression estimator of the coefficients $\beta_{\op{RR}}$ is:
 \begin{align*}
 	\beta_{\op{RR}}				&=	(X X^T + \lambda I)^{-1}	X y	, \; \lambda > 0	\\
	X X^T + \lambda I			&=	\MAT{1+ \lambda & \alpha \\ \alpha  &1+ \lambda} \\
	(X X^T + \lambda I)^{-1}		&=	\frac{1}{(1 + \lambda)^2 - \alpha^2} \MAT{1+ \lambda & -\alpha \\ -\alpha & 1+ \lambda} \\	
	\Rightarrow \beta_{\op{RR}}	&=	\frac{1}{(1 + \lambda)^2 - \alpha^2} \MAT{1+ \lambda & -\alpha \\ -\alpha & 1+ \lambda} \MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1} \\
							&=	\frac{1}{(1 + \lambda)^2 - \alpha^2}  \MAT{(1+ \lambda) (\beta_{\op{true}} + 0.1) -\alpha^2  \beta_{\op{true}} -0.1 \alpha \\
																-\alpha \beta_{\op{true}}  -0.1 \alpha + \alpha (1+\lambda) \beta_{\op{true}}  + 0.1 (1 + \lambda) } \\
							&=	\frac{1}{(1 + \lambda)^2 - \alpha^2}  \MAT{(1+ \lambda  -\alpha^2) \beta_{\op{true}}  + 0.1 (1+ \lambda  -\alpha) \\ 			
																\alpha \lambda \beta_{\op{true}} + 0.1 (1+ \lambda  -\alpha)} \\
							&=	 \MAT{\frac{(1+ \lambda  -\alpha^2) \beta_{\op{true}}} {(1 + \lambda)^2 - \alpha^2} + \frac{0.1} {1 + \lambda + \alpha} \\
									\frac{ \lambda \alpha  \beta_{\op{true}}} {(1 + \lambda)^2 - \alpha^2} + \frac{0.1} {1 + \lambda + \alpha} } \\
							&_{\alpha \rightarrow 1}\rightarrow \frac{ \beta_{\op{true}} + 0.1}{ 2 + \lambda} \MAT{1  \\ 1} 
 \end{align*} 
If we compare to $\beta_{\op{OLS}}$ estimator,  the ridge regression estimator has coefficients on both $w_1$ and $w_2$ and this coefficient can be regularized using $\lambda$. 
When the data gives no reason to choose between different linear combinations of colinear features (we did not that know they are correlated) , ridge estimator chooses equal weighting. 
Also even if $XX^T$ is singular (rank $1$), the l$_2$ regularization 
using $\lambda$ makes the matrix $XX^T  + \lambda I$ non singular (with a value of $\lambda$ which prevents its determinant to be zero)
and we are still able to find a solution which includes both features. In addition we have a new set of coefficients each time we tune $\lambda$:
when $\lambda \rightarrow 0, \beta_{\op{RR}} \rightarrow (\frac{\beta_{\op{OLS}}} {2} + 0.05) [ 1 \; 1]^T$ up to $\lambda \rightarrow \infty, \beta_{\op{RR}} \rightarrow [0 \; 0]^T$. 
 
  \item  What does the corresponding estimate of the response $y_{\op{RR}} := X^T\beta_{\op{RR}}$ equal to when $\alpha \rightarrow 1$? Is it collinear with the true feature $w_1$?\\
  \medskip
  When  $\alpha \rightarrow 1, w_2 \rightarrow w_1$, which leads to
 \begin{align*}
 	y_{\op{RR}} := X^T\beta_{\op{RR}}	&=	\frac{ \beta_{\op{true}} + 0.1}{ 2 + \lambda} [w_1 w_2]  \MAT{1  \\ 1} \\
								&= 	\frac{ \beta_{\op{true}} + 0.1}{ 2 + \lambda} [w_1 w_1] \MAT{1  \\ 1} \\
								&= 	2 \;  \frac{\beta_{\op{true}} + 0.1}{ 2 + \lambda} \; w_1
 \end{align*}    
  $y_{\op{RR}}$ is collinear with the true feature $w_1$.
  Compared to $y_{\op{OLS}}$, the parameter $\lambda$ allows to control the amount of linearity between the response and control variable 
  (when $\lambda=0, y_{\op{RR}} = y_{\op{OLS}}$), which might be desirable if the test data points, not known in advance, are not totally dependent on the feature $w_1$.
  
  \end{enumerate} 
 
 \newpage
 
 \item (Prior knowledge) Consider a linear regression problem where we have prior information indicating that the coefficients should be close to a certain value $\beta_{\op{prior}}$. 
 \begin{enumerate}
 
   \item How can you incorporate this prior knowledge if you are using ridge regression? Write the corresponding optimization problem.\\
 
  \medskip
   If we want to include that the coefficients should be close to $\beta_{\op{prior}}$, the ridge-regression estimator is the minimizer of the optimization problem:
   $$\beta_{\op{RR}} := \argmin_\beta \| y - X^T \beta \|_2^2 + \lambda \| \beta - \beta_{\op{prior}} \|_2^2$$
   where $\lambda > 0$ is a fixed regularization parameter.
   
   \item Assume that the data are generated according to a linear model $\rnd{y}:= X^T \beta_{\op{true}} + \rnd{z}$, where $\beta_{true}\in \R^{p}$ and $X  \in \R^{p \times n}$ are fixed and $\rnd{z}$ is an iid Gaussian random vector with zero mean and variance $\sigma^2$. Does the modification change the mean or the covariance matrix of the estimator? If so, report the new value.\\
   
\medskip
 
 \begin{align*}
 	\| y - X^T \beta \|_2^2		&=	(y - X^T \beta)^T (y - X^T \beta) + \lambda (\beta - \beta_{\op{prior}})^T (\beta - \beta_{\op{prior}}) \\
							&=	(y^T - \beta^T X)  (y - X^T \beta) + \lambda (\beta^T - \beta_{\op{prior}}^T) (\beta - \beta_{\op{prior}}) \\
							&=	\beta^T X X^T \beta -2 (X y)^T \beta + y^T y + \lambda \big( \beta^T \beta -2  \beta_{\op{prior}}^T \beta +  \beta_{\op{prior}}^T \beta_{\op{prior}} \big) \\
							&=	\beta^T X X^T \beta -2 (X y)^T \beta + \lambda \big( \beta^T \beta -2  \beta_{\op{prior}}^T \beta) +  y^T y +  \lambda  \beta_{\op{prior}}^T \beta_{\op{prior}} := f(\beta)
 \end{align*}    
 $f$ is a quadratic form in $\beta$ and its gradient and Hessian equal:
 \begin{align*}
 	\nabla_{\beta} f(\beta)		&=	2 (X X^T \beta - X y +  \lambda (\beta - \beta_{\op{prior}}))  \\
	\nabla_{\beta}^2 f(\beta)		&=	2 (X X^T  +  \lambda I) 
 \end{align*}    
  $v^T (X X^T  +  \lambda I) v  = v^T X X^T v + \lambda v^T v = \|X^T v \|_2^2 + \lambda \| v \|_2^2 \ge 0$ and it is zero only when $v$ is the zero vector (by property of the norm operator), thus the matrix $X X^T  +  \lambda I$ is positive definite
  and invertible. The unique minimum can be found by setting the gradient to zero, which gives that the ridge regression estimator is:
  
  $$\beta_{\op{RR}}  =   (X X^T  +  \lambda I)^{-1} (X y + \lambda \beta_{\op{prior}})$$
  with mean:
\begin{align*}
  	\E[\beta_{\op{RR}} ]		&= 		\E[ (X X^T  +  \lambda I)^{-1} (X X^T \beta_{\op{true}} + X \rnd{z} +  \lambda \beta_{\op{prior}})] \\
						&=		 (X X^T  +  \lambda I)^{-1} (X X^T \beta_{\op{true}}  + \lambda \beta_{\op{prior}}) \\
						&	\text{ ~ by linearity of the expectation and } \rnd{z} \text{ has zero mean}
\end{align*} 
Let $X=U S V^T$ the SVD of X, if $X$ is full rank, and $p \le n$, $U$ is square, $UU^T= U^TU = I$,we can then expand the previous expression:
\begin{align*}
	\E[\tilde{\beta}_{\op{RR}} ]		&=	(U S^2 U^T + \lambda U \; U^T)^{-1}	(U S^2 U^T \beta_{\op{true}}  + \lambda \beta_{\op{prior}}) \\
							&=	U (S^2 + \lambda I)^{-1} S^2 U^T  \beta_{\op{true}}  + U (S^2 + \lambda I)^{-1}  \lambda \beta_{\op{prior}} U^T \\
							&=	\sum_{i=1}^p \bigg( \frac{s_i^2 \PROD{u_i} {\beta_{\op{true}} } + \PROD{u_i} { \lambda \beta_{\op{prior}}}} {s_i^2 + \lambda} \bigg) u_i \\
							&=	\sum_{i=1}^p \bigg(   \frac{ \PROD{u_i} {s_i^2 \beta_{\op{true}} +  \lambda \beta_{\op{prior}} }}  {s_i^2 + \lambda} \bigg) u_i
\end{align*} 

and variance:	
\begin{align*}
	\Var(\tilde{\beta}_{\op{RR}})	&=	\Var( (X X^T  +  \lambda I)^{-1} (X y + \lambda \beta_{\op{prior}})) \\
						&=	\Var( (X X^T  +  \lambda I)^{-1}  X y + \lambda  \beta_{\op{prior}} (X X^T  +  \lambda I)^{-1} ) \\
						&=	\Var( (X X^T  +  \lambda I)^{-1}  X y) \\
						&=	(X X^T  +  \lambda I)^{-1}  X \var(y)	((X X^T  +  \lambda I)^{-1}  X)^T \\
						&=	\sigma^2 (X X^T  +  \lambda I)^{-1}  X X^T (X X^T  +  \lambda I)^{-1} \\
						&=	U \sigma^2 (S^2 + \lambda I)^{-1} S^2 (S^2 + \lambda I)^{-1} U^T \\
						&= 	\sigma^2 	U  \text{diag}_{i=1}^p \bigg( \frac{s_i^2} {(s_i^2 + \lambda)^2} \bigg) 	U^T
\end{align*}					
   
   The mean has a systematic bias which is proportional to $\lambda$ and  $\beta_{\op{prior}}$. 
   If $\lambda >> s_i^2$ then $\frac{\lambda}{s_i^2} >> 1$ and $\E[\tilde{\beta}_{\op{RR}} ]	$ is proportional to $\beta_{\op{prior}} $, 
   on the other hand if $\frac{\lambda}{s_i^2} << 1$ then $\E[\tilde{\beta}_{\op{RR}} ]	$ is proportional to $\beta_{\op{true}}$.
   When $\lambda = 0$ then $\E[\tilde{\beta}_{\op{RR}} ] = \beta_{\op{true}}$ and $\Var(\tilde{\beta}_{\op{RR}}) = \sigma^2 I$,
   and when $\lambda \rightarrow \infty$, $\E[\tilde{\beta}_{\op{RR}} ] \rightarrow  \beta_{\op{prior}}$ and $\Var(\tilde{\beta}_{\op{RR}}) \rightarrow 0_{p \times p}$.
   $\lambda$ controls how much prior $\beta_{\op{prior}}$ we want to consider.
   The variance has the same expression as the ridge regression estimator without $\beta_{\op{prior}}$ thus the same behavior in regard of the action of the parameter $\lambda$ 
   (see linear regression notes, from equation 139 to the end of the chapter about ridge regression).
   Compare to the OLS estimator the regularization parameter $\lambda$ can cancel out the high variance due to very small singular values.
   
    \item How can you incorporate this prior knowledge if you are using gradient descent with early stopping? Write the corresponding update equation as a function of $\beta_{\op{prior}}$.\\

\medskip				
We will follow the reasoning of the linear regression notes with one difference, the starting point for $\beta^{(0)}$ is $\beta_{\op{prior}}$
(for an alternate version using the initial quadratic form of part (a), if interested, please see at the end of this homework, one reason to follow this path is that we obtain an algorithm achieving similar
objectives but easier to analyze).


For $X  \in \R^{p \times n}$ and a response vector $y \in \R^n, \beta \in \R^p$, from part (b) we know that the gradient equals $$\nabla_{\beta} f(\beta) =	2 (X X^T \beta - X y )$$
The gradient-descent updates are:
\begin{align*}
		\beta^{(k+1)}	&=	\beta^{(k)} - \; \alpha_k \nabla_{\beta} f(\beta^{(k)})	\\
					&=	\beta^{(k)}  - \alpha_k (X X^T \beta^{(k)} - X y) \\
					&=    \beta^{(k)}  +  \alpha_k  X (y   - X^T  \beta^{(k)})\\
					&=	 \beta^{(k)}  +  \alpha_k  \sum_{i=1}^n (y[i] - \PROD{x_i}{\beta^{(k)}}) x_i)
\end{align*}
   where $\beta^{(k)} \in \R^p$ and $ \alpha_k > 0$ which are the estimated step size at iteration $k$. Note that for the term corresponding to $\alpha_k$, at  step $k$, for the next $\beta^{(k+1)}$ 
   we want to reduce the error with the response variable $y$, so we add or subtract a small multiple of $x^{(i)}$ accordingly. 
   
    \item Assume that the data are generated according to the linear model described above. Does the modification change the mean or the covariance matrix of the estimator? If so, report the new value.\\
    
\medskip

Assuming constant step size, we can express the previous expression as:
\begin{align*}
		\beta^{(k+1)}	&=	( I - \alpha X X^T ) \beta^{(k)} +  \alpha X y \\
					&=	(I - \alpha X X^T)^{k+1} \beta_{\op{prior}}	+ \sum_{i=0}^k (I - \alpha X X^T)^i \alpha X y 
\end{align*}

Assuming $X$ full rank and p $\le$ n, $U$ is square and, $UU^T = U^TU = I$, let $X = USV^T$, we have then:
\begin{align*}
		\beta^{(k+1)}	&=	(UU^T - \alpha US^2 U^T)^{k+1} \beta_{\op{prior}}	+ \alpha \sum_{i=0}^k      (UU^T - \alpha US^2U^T)^i U S V^T y \\
					&=    U (I - \alpha S^2)^{k+1}  U^T  \beta_{\op{prior}}		+ \alpha U \sum_{i=0}^k  (I - \alpha S^2)^i S V^T y \\
					&=	U \text{diag}_{j=1}^p  (1 - \alpha s_j^2)^{k+1} U^T   \beta_{\op{prior}} + \alpha \; U  \;  \text{diag}_{j=1}^p \bigg( \sum_{i=0}^k (1 - \alpha s_j^2 )^i \bigg)  S V^T y \\
					& \text{ ~ using the geometric-sum formula: } \\
					&= 	U \text{diag}_{j=1}^p  (1 - \alpha s_j^2)^{k+1} U^T   \beta_{\op{prior}} + \alpha \; U  \;  \text{diag}_{j=1}^p \bigg( \frac{ 1 - (1 - \alpha s_j^2 )^{k+1}} {\alpha s_j^2} \bigg)  S V^T y \\
					&=    U \text{diag}_{j=1}^p  (1 - \alpha s_j^2)^{k+1} U^T   \beta_{\op{prior}} + U  \;  \text{diag}_{j=1}^p \bigg( \frac{ 1 - (1 - \alpha s_j^2 )^{k+1}} {s_j} \bigg)  V^T y \\
\end{align*}

Let $\nu_j := 1 - \alpha s_j^2$, and starting at $ \beta_{\op{prior}}$, then we now have
\begin{align*}
	\rnd{\beta}^{(k)} 	&= 	  U \text{diag}_{j=1}^p   \nu_j^k \; U^T   \beta_{\op{prior}} + U \;    \text{diag}_{j=1}^p \big(  \frac{ 1 - \nu_j^k} {s_j}  \big) (V^T X^T  \beta_{\op{true}}  + V^T  \rnd{z})\\
					&= 	  U \text{diag}_{j=1}^p   \nu_j^k \; U^T   \beta_{\op{prior}} + U \;    \text{diag}_{j=1}^p \big(  \frac{ 1 - \nu_j^k} {s_j}  \big) (V^T V S U^T \beta_{\op{true}}  + V^T  \rnd{z})\\
					&= 	  U \text{diag}_{j=1}^p   \nu_j^k \; U^T   \beta_{\op{prior}} + U \;    \text{diag}_{j=1}^p \big(  \frac{ 1 - \nu_j^k} {s_j}  \big) (S U^T \beta_{\op{true}}  + V^T  \rnd{z})\\
					&= 	  U \text{diag}_{j=1}^p   \nu_j^k \; U^T   \beta_{\op{prior}} + U \;      \text{diag}_{j=1}^p \big( 1 - \nu_j^k \big)  U^T \beta_{\op{true}} + U  \text{diag}_{j=1}^p \frac{ 1 - \nu_j^k} {s_j}  V^T  \rnd{z}
\end{align*}	
 Then using Theorem 8.6 in the PCA lecture notes, we have a Gaussian random vector  with mean:
\begin{align*}
	\beta_{\text{GD}} 		&= \sum_{j=1}^p \PROD{u_j} {(1 - \nu_j^k)  \beta_{\op{true}} + \nu_j^k \beta_{\op{prior}} } u_j \\
						&= \sum_{j=1}^p \PROD{u_j} {(1 - (1 - \alpha s_j^2)^k) \beta_{\op{true}} + (1 - \alpha s_j^2)^k \beta_{\op{prior}} } u_j \\
\end{align*}
and variance:
\begin{align*} 
	\Sigma_{\text{GD}}			&= \sigma^2 U  \text{diag}_{j=1}^p \frac{1 - \nu_j^k} {s_j}  V^T V  \text{diag}_{j=1}^p \frac{ 1 - \nu_j^k} {s_j}  U^T \\
							&= \sigma^2 U   \text{diag}_{j=1}^p \frac{(1 - \nu_j^k)^2} {s_j^2} \; U^T \\
							&= \sigma^2 U   \text{diag}_{j=1}^p \frac{ (1 - (1 - \alpha s_j^2)^k)^2}  {s_j^2} \; U^T \\
\end{align*}
For small $k$ and $\alpha s_j, (1 - \alpha s_j^2)^k \approx 1 - \alpha k s_j^2, 1 - (1 - \alpha s_j^2)^k \approx  \alpha k s_j^2$ (since $x \approx 0, (1-x)^k \approx 1 -k x$)
thus $\beta_{\text{GD}} \approx  \sum_{j=1}^p \PROD{u_j} {(1 - \alpha k s_j^2) \beta_{\op{prior}}} u_j$, the bias is closer to $\beta_{\op{prior}}$, which we may prefer.
And if we select a step size $\alpha$ mall enough: $0 < \alpha < \frac{2} {s_1^2} \le \frac{2} {s_j^2} \rightarrow |1 - \alpha s_j^2| < 1 \rightarrow \lim_{k \rightarrow \infty} (1 - \alpha s_j^2)^k =0, j=1, \dots ,p $ then
the mean estimate $\beta_{\text{GD}}$ converges to $\beta_{\op{true}}$ as k increases.
At the same time, at iteration $k$, the variance in the direction of the $j$th left singular vector equals:
	$$\frac{\sigma^2 (1 - (1 - \alpha s_j^2)^k)^2}  {s_j^2}$$
Then as $k$ increases, the variance approaches eventually $\frac{\sigma^2}{s_j^2}$, as in OLS, it also increases.
Like in ridge regression there is an optimal value of $k$ which optimizes the bias-variance tradeoff (this value of $k$ is identified during cross-validation and using early stopping).
 
 \end{enumerate}
 
\item The code you will implement in this question is located in the
  \verb|regress.py| file in the time folder of  \verb|hw5.zip|.
  Define a sequence of random variables as follows:
  \begin{align*}
    \rvx [0] &= 1\\
    \rvx[1] &= \rvx[0]+\rvz[1]\\
    \rvx[2] &= \rvx[1]+\rvz[2]\\
    \rvx[3] &= \rvx[2]+\rvz[3]\\
    \rvx[4] &= \rvx[3]+\rvz[4],
  \end{align*}
  where $\rvz[1],\rvz[2],\rvz[3],\rvz[4]$ are independent,
  $\rvz[1]\sim\Norm(0,1)$ and
  $\rvz[2],\rvz[3],\rvz[4]\sim\Norm(0,0.01^2)$.  There is a function
  $f:\RR^5\to\RR$ of the form $f(x)=\vbeta^Tx$ where $\vbeta$ is
  unknown.  We are given a training sample of independent draws
  $$(\rvx_1,f(\rvx_1)+\rw_1),\ldots,(\rvx_n,f(\rvx_n)+\rw_n)\in\RR^5\times\RR,$$
  where $\rw_i$ are iid standard normal random variables corrupting
  our measurements of $f$.
  Using this training data, we will estimate $\vbeta$ and test our
  performance on a validation set drawn from the same distribution.
  Below we refer to the square loss function
  $L:\RR^5\times\RR^{n\times 5}\times\RR^n\to\RR$ defined by
  $$L(\hat{\beta},X,y) = \sum_{i=1}^n (X[i,:]\hat{\beta}-y[i])^2$$
  where $X\in\RR^{n\times 5}$ denotes a matrix of data (training or validation; each row
  is a data point), and $y$ is the corresponding vector of $f$-values.
  \begin{enumerate}
  \item Using least squares (i.e., minimizing the square loss on the
    training set) compute an estimate for $\vbeta$.  Include your
    estimate for $\vbeta$, your square loss on the training set, and
    your square loss on the validation set in your submission.
    [Hint: If computed correctly your training loss should be larger
      than 30 and your validation loss should be larger than 10.]
      
   Using least squares we obtain for $\vbeta$:
   \begin{center}
    		\begin{tabular}{ | c | c | c | c | c | c | }
		\hline
			9.506 & 39.07 & -8.181 & -23.083 &  -4.189\\ 
		\hline
    	\end{tabular}
    \end{center}
    And the square losses are:
    \begin{enumerate}[(a)]
	\item Training square loss:34.743
	\item Validation square loss:11.283
    \end{enumerate}

  \item Compute the singular values of the training data matrix
    $X\in\RR^{n\times 5}$.\\
    
   The singular value of the data ($n \times 5$) are:
   \begin{center}
    		\begin{tabular}{ | c | c | c | c | c | c | }
		\hline
			24.812 & 5.573 & 0.102 & 0.059 & 0.038\\ 
		\hline
    	\end{tabular}
    \end{center}
    
   The singular value of the training data ($n_\text{train} \times 5$) are: 
   \begin{center}
    		\begin{tabular}{ | c | c | c | c | c | c | }
		\hline
			21.104 & 4.558 & 0.092 & 0.048 &  0.034\\ 
		\hline
    	\end{tabular}
    \end{center}

In both cases we notice that there is a large difference in amplitude between the first singular value and the rest of the singular values.
And this reflects the data generation process, the four data points in a row are strongly correlated each other, and as we move up the column for a specific row, 
the data point is more and more correlated with the previous ones:
the second datapoint is the first one and some noise, the third datapoint is the second one with some noise, and so until the fifth datapoint.

  \item The true value of $\vbeta$ can be found at the top of
    \verb|regress.py|.  Give an explanation as to why the least squares
    estimates aren't close to the true $\vbeta$-values.
    
   True $\vbeta$
   \begin{center}
    		\begin{tabular}{ | c | c | c | c | c | c | }
		\hline
			9 & 1 & 1 & 1 &  1\\ 
		\hline
    	\end{tabular}
    \end{center}

  OLS $\vbeta$:
   \begin{center}
    		\begin{tabular}{ | c | c | c | c | c | c | }
		\hline
			9.506 & 39.07 & -8.181 & -23.083 &  -4.189\\ 
		\hline
    	\end{tabular}
    \end{center}

We just saw that some of the singular values of the training data matrix are minuscule. 
Estimating the contribution of low-variance components requires to amplify the linear coefficients ($\vbeta$ ).
    
  \item Use ridge regression to produce a new estimate of $\vbeta$
    and report the resulting estimate of $\vbeta$, and
    your square loss on the training and validation sets.  Here
    $\hat{\beta}$ should solve
    $$\text{minimize}_{\veta}\quad \|X\veta-\vy\|_2^2 + 0.5\|\veta\|_2^2.$$
    
    Using ridge regression and a regularization parameter of $\lambda=0.5$ we obtain for $\vbeta$:
     \begin{center}
    		\begin{tabular}{ | c | c | c | c | c | c | }
		\hline
			9.259 & 1.240 & 0.992 & 0.818 &  0.71 \\ 
		\hline
    	\end{tabular}
    \end{center}
    And the square losses are:
    \begin{enumerate}[(a)]
	\item Training square loss:43.280
	\item Validation square loss:6.075
    \end{enumerate}
  
  These results are expected as ridge-regression estimator controls the magnitude of the coefficients with the regularization parameter $\lambda$ (equals to $0.5$),
  it neutralizes the contribution of the small singular values: $\lambda >> $ small singular values (canceling the variance in the direction of the corresponding singular vectors).
  We could have selected $\lambda$ using cross-validation (trying different values of $\lambda$ and select the best value), 
  but  when we compare the ridge-regression estimator performance to the least-squares estimator (part a),
  it seems this particular value of $\lambda$ is somewhat optimal providing a good estimation of the true coefficients
   $\vbeta$ and at same time improving the validation error (reducing the square loss validation error).
  

  \end{enumerate}
You're not required to include your code in your submission, but you are free to do so. 
 \end{enumerate}
 
 \newpage
 
 Other answers to question 3 c) and d):
 \begin{enumerate}
 
     \item How can you incorporate this prior knowledge if you are using gradient descent with early stopping? Write the corresponding update equation as a function of $\beta_{\op{prior}}$.\\

\medskip				

For $X  \in \R^{p \times n}$ and a response vector $y \in \R^n, \beta \in \R^p$, from part (b) we know that the gradient equals $$\nabla_{\beta} f(\beta) =	2 (X X^T \beta - X y +  \lambda (\beta - \beta_{\op{prior}}))$$
The gradient-descent updates are:
\begin{align*}
		\beta^{(k+1)}	&=	\beta^{(k)} - \; \alpha_k \nabla_{\beta} f(\beta^{(k)})	\\
					&=	\beta^{(k)}  - \alpha_k (X X^T \beta^{(k)} - X y +  \lambda (\beta^{(k)} - \beta_{\op{prior}})) \\
					&=    ((1- \lambda \alpha_k) I  - \alpha_k X X^T) \beta^{(k)}  +  \alpha_k  X y  + \lambda  \alpha_k  \beta_{\op{prior}} \\
					&=	(I -  \alpha_k (\lambda +  X X^T)) \beta^{(k)}  +  \alpha_k  X y  + \lambda  \alpha_k  \beta_{\op{prior}} \\
					&=	\beta^{(k)}  + \alpha_k  \big( X y  - X X^T \beta^{(k)}   + \lambda (\beta_{\op{prior}} - \beta^{(k)}) \big) \\
					&=	 \beta^{(k)}  +  \alpha_k  \big( \sum_{i=1}^n (y[i] - \PROD{x_i}{\beta^{(k)}}) x_i - \lambda ( \beta^{(k)}  -   \beta_{\op{prior}}) \big)
\end{align*}
   where $\beta^{(k)} \in \R^p$ and $ \alpha_k > 0$ which are the estimated step size at iteration $k$. Note that for the term corresponding to $\alpha_k$, at  step $k$, for the next $\beta^{(k+1)}$ 
   we want to reduce the error with the response variable $y$  and at the same time to be close to the prior $\beta_{\op{prior}}$, the last term being controlled by the regularization parameter $\lambda$.
   
    \item Assume that the data are generated according to the linear model described above. Does the modification change the mean or the covariance matrix of the estimator? If so, report the new value.\\
    
\medskip

Assuming constant step size, we can express the previous expression as:
\begin{align*}
		\beta^{(k+1)}	&=	\big( (1 - \lambda \alpha)  I - \alpha X X^T \big) \beta^{(k)}		+  	\alpha (X y + \lambda \beta_{\op{prior}}) \\
					&=	\big(I - \alpha (\lambda I + X X^T)  \big)^{k+1} \beta^{(0)}		+	\sum_{i=0}^k \big(I - \alpha (\lambda I + X X^T)  \big)^i \alpha (X y + \lambda \beta_{\op{prior}})
\end{align*}

Assuming $X$ full rank and p $\le$ n, $U$ is square and, $UU^T = U^TU = I$, let $X = USV^T$, we have then:
\begin{align*}
		\beta^{(k+1)}	&=	U (I - \alpha (S^2 + \lambda I))^{k+1} U^T \beta^{(0)}		+ \alpha \sum_{i=0}^k 	U (I - \alpha (S^2 + \lambda I))^ i U^T (U S V^T y +  \lambda \beta_{\op{prior}}) \\
					&=	U (I - \alpha (S^2 + \lambda I))^{k+1} U^T \beta^{(0)}		+ \alpha \sum_{i=0}^k 	U (I - \alpha (S^2 + \lambda I))^ i (S V^T y + \lambda \beta_{\op{prior}}  U^T) \\
					&=	U \text{diag}_{j=1}^p \bigg( (1 - \alpha (s_j^2 + \lambda))^{k+1} \bigg)U^T \beta^{(0)}	\\
					&+ \alpha \; U  \;  \text{diag}_{j=1}^p \bigg( \sum_{i=0}^k (1 - \alpha (s_j^2 + \lambda))^i \bigg)  (S V^T y + \lambda \beta_{\op{prior}}  U^T) \\
					&= U \text{diag}_{j=1}^p \bigg( (1 - \alpha (s_j^2 + \lambda))^{k+1} \bigg)U^T \beta^{(0)}	\\
					&+ U  \;  \text{diag}_{j=1}^p \bigg( \frac{ 1 - (1 - \alpha (s_j^2 + \lambda))^{k+1}} {s_j^2 + \lambda} \bigg)  (S V^T y + \lambda \beta_{\op{prior}}  U^T)
\end{align*}
If step size $\alpha$ is small enough: $0 < \alpha < \frac{2} {\lambda + s_1^2} \le \frac{2} {\lambda + s_j^2} \rightarrow |1 - \alpha (s_j^2 + \lambda)| < 1 \rightarrow \lim_{k \rightarrow \infty} (1 - \alpha (s_j^2 + \lambda))^k =0, j=1, \dots ,p $ then gradient descent converges to:
$$\lim_{k \rightarrow \infty} \beta^{(k+1)} = U  \;  \text{diag}_{j=1}^p \bigg( \frac{1} {s_j^2 + \lambda} \bigg)  (S V^T y + \lambda \beta_{\op{prior}}  U^T)$$

Let $\nu_j := 1 - \alpha (s_j^2 + \lambda)$, and starting at $\beta^{(0)}=0$, then we now have
\begin{align*}
	\rnd{\beta}^{(k)} 	&= 	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{ 1 - \nu_j^k} {s_j^2 + \lambda}  \bigg) (S V^T X^T  \beta_{\op{true}}  + S V^T  \rnd{z} +  \lambda \beta_{\op{prior}}  U^T)\\
					&=	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{ 1 - \nu_j^k} {s_j^2 + \lambda}  \bigg) (S^2 U^T  \beta_{\op{true}}  +  \lambda \beta_{\op{prior}}  U^T  + S V^T  \rnd{z}) \\	
					&=	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{s_j^2  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) U^T \beta_{\op{true}}  + \lambda U \;    \text{diag}_{j=1}^p \bigg(  \frac{1 - \nu_j^k} {s_j^2 + \lambda} \bigg) \beta_{\op{prior}} U^T \\
					&+  	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{ s_j  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) V^T  \rnd{z} 
\end{align*}	
 Then using Theorem 8.6 in the PCA lecture notes, we have a Gaussian random vector  with mean:
\begin{align*}
	\beta_{\text{bias\_GD}} 	&= \sum_{i=1}^p \frac{1 - \nu_j^k}{s_i^2 + \lambda} \PROD{u_i} {s_i^2 \beta_{\op{true}}+ \lambda  \beta_{\op{prior}} } u_i \\
						&= \sum_{i=1}^p \frac{1 - (1 - \alpha (s_i^2 + \lambda))^k} {s_i^2 + \lambda} \PROD{u_i} {s_i^2 \beta_{\op{true}}+ \lambda  \beta_{\op{prior}} } u_i \
\end{align*}
and variance:
\begin{align*} 
	\Sigma_{\text{GD}}			&= \sigma^2 U  \text{diag}_{j=1}^p \bigg(  \frac{ s_j  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) V^T V  \text{diag}_{j=1}^p \bigg(  \frac{ s_j  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) U^T \\
							&= \sigma^2 U  \text{diag}_{j=1}^p \bigg(  \frac{ s_j^2  (1 - (1 - \alpha (s_j^2 + \lambda))^k)^2} {(s_j^2 + \lambda)^2} \bigg) U^T
\end{align*}
 Note that for small step size $\alpha$ (see previous condition depending on $\lambda$ and $s_i$) and large $k$  enough, 
 we find the same expressions for the mean and variance that we had  for the ridge regression estimator in part (b) (since for $k >> 1: \nu_j \rightarrow 0$) .
 Therefore  we reach the same conclusion for limiting behavior and the usage of the regularization parameter $\lambda$ that we obtained in part (b).
 \end{enumerate}

\end{document}
