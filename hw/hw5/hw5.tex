\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}

\input{macros}
%\input{Symbols}

\begin{document}

\noindent DS-GA.1013 Mathematical Tools for Data Science \\
Homework 5\\
Yves Greatti - yg390\\

\begin{enumerate}

\item (Augmented dataset) Ridge regression is equivalent to applying OLS on an expanded dataset that has additional examples. Describe these additional examples in detail. Intuitively, what effect do these additional examples have?\\
\medskip

The ridge regression estimate is defined as $\beta_{\op{RR}} = (X X^T + \lambda I)^{-1} X y$, where $X \in \R^{p \times n}, y \in \R^n, \beta \in \R^p$, which can be reformulated as a modified least-square problem
$$\beta_{\op{RR}} = \argmin_\beta \big\| \begin{bmatrix} y   \\ 0 \end{bmatrix} - \begin{bmatrix} X^T   \\ \sqrt{\lambda} I_{n \times p} \end{bmatrix} \beta \big\|_2^2$$
It seems like we have added $p$ vectors to the original $n$ datapoints $[x_i, \ldots, x_n], x_i \in \R^p, i=1, \ldots, n$. By doing so, Ridge regression embedded the original problem in $\R^n$ into a larger space $\R^{n+p}$ by moving into $p$ different 
directions with a small amount $\sqrt{\lambda}$ which could decrease any collinearity present in the original data points $X$. As we have seen in the notes of linear regression, when the number of training data is small, $\lambda$ neutralizes the large variance of the errors between the true  $\beta$ coefficients and the ridge regression coefficients due to small singular values  of the sample covariance matrix.

\item  (Correlated features) Consider a regression problem where the response only depends on one feature, but we don't know it, so we incorporate an additional feature into the model that happens to be very correlated with the first feature. More specifically, let $y \in \R^{n}$ be defined by
\begin{align}
y := \beta_{\op{true}} w_1 + z, 
\end{align}
where $\beta_{\op{true}} \in \R$ is the true coefficient, $w_1 \in \R^n$ is the first feature vector, and $z \in \R^{n}$ is additive noise. The second feature vector is given by $w_2\in \R^n$ and can be decomposed into
\begin{align}
w_2 = \alpha w_1 + \sqrt{1-\alpha^2} w_{\perp},
\end{align}  
where $w_{\perp}$ is orthogonal to $w_1$. The vectors $w_1$, $w_2$, $w_{\perp}$ and $z$ all have unit $\ell_2$ norm. In addition, we assume
\begin{align}
w_1^Tz = 0.1, \\
w_{\perp}^Tz = 0.1.
\end{align}
We fit a linear regression model to $y$ using the feature matrix 
  \begin{align}
  X & = \MAT{w_1^T \\ w_2^T}.
  \end{align} 
 \begin{enumerate}
  \item What does the OLS estimator of the coefficients $\beta_{\op{OLS}}$ equal to when $\alpha \rightarrow 1$? Explain what is happening. \\
\emph{Hint:} Use the fact that for any $a$, $b$, $c$, and $d$ such that $ad \neq bc$
\begin{align}
 \MAT{a & b \\ c & d}^{-1} = \frac{1}{ad-bc} \MAT{d & -b \\ -c & a}.
\end{align}\\

\medskip

The OLS estimator of the coefficients of $\beta_{\op{OLS}}$  is given by $\beta_{\op{OLS}} = (X X^T)^{-1} X y$.
Expanding each term, we have:
\begin{align*}
 	(X X^T)				&=	\MAT{w_1^T \\ w_2^T} [w_1 \; w_2]	\\
						&=	\MAT{w_1^T w_1 & w_1^T w_2 \\ w_2^T w_1 & w_2^T w_2}	\\
						&= 	\MAT{\|w_1\|_2^2 & w_1^T w_2 \\ w_2^T w_1  & \|w_2\|_2^2} \\
	w_1^T w_2			&=	w_1^T (\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp}) \\
						&=	\alpha \|w_1\|_2^2 + \sqrt{1-\alpha^2}  w_1^T w_{\perp} \\
						&=	\alpha \text{ ~ since } \|w_1\|_2 = 1 \;, w_1 \perp  w_{\perp} \\
	w_2^T w_2			&= 	(\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp})^T (\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp}) \\
						&= 	\alpha^2 w_1^T w_1 + 2 \alpha \sqrt{1-\alpha^2} w_1^T w_{\perp} + (1-\alpha^2) w_{\perp}^T w_{\perp} \\
						&=	\alpha^2 \| w_1 \|_2^2 +  (1-\alpha^2) \|w_{\perp}\|_2^2 \text{ ~  knowing that }  w_{\perp} \perp w_1 \\
						&=	\alpha^2 +  (1-\alpha^2) = 1 \; \text{by assumptions } \| w_1 \|_2 = \|w_{\perp}\|_2 = 1 \\
	\Rightarrow (X X^T)		&= 	\MAT{1 & \alpha \\ \alpha & 1} \\
	\Rightarrow (X X^T)^{-1}	&=	\frac{1}{1-\alpha^2} \MAT{1 & -\alpha \\ -\alpha  & 1} \\
	X y					&= 	\MAT{w_1^T \\ w_2^T} (\beta_{\op{true}} w_1 + z) \\
						&=	\MAT{\beta_{\op{true}} w_1^T w_1 + w_1^T z \\ \beta_{\op{true}} w_2^T w_1 + w_2^T z } \\
						&=	\MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1}	
\end{align*}
Substituting each of the previous terms back into the expression of $\beta_{\op{OLS}}$, we find that:
\begin{align*}
	\beta_{\op{OLS}}		&=	\frac{1}{1-\alpha^2} \MAT{1 & -\alpha \\ -\alpha  & 1}  \MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1} \\
						&= 	\frac{1}{1-\alpha^2}  \MAT{\beta_{\op{true}} + 0.1 -\alpha^2 \beta_{\op{true}} -0.1 \alpha \\ -\alpha \beta_{\op{true}} -0.1 \alpha + \alpha \beta_{\op{true}}  + 0.1} \\
						&=	\frac{1}{1-\alpha^2}  \MAT{(1-\alpha^2)  \beta_{\op{true}} + 0.1 (1-\alpha) \\ 0.1 (1 - \alpha)} \\
						&=  	\MAT{\beta_{\op{true}} + \frac{0.1} {1 + \alpha} \\ \frac{0.1} {1 + \alpha}}
\end{align*}
When $\alpha \rightarrow 1$ , $\beta_{\op{OLS}} \rightarrow   \MAT{\beta_{\op{true}} + 0.05  \\ 0.05}$.
The OLS estimator ignore the correlated feature $w_2$ and adds a fixed bias $0.05$ to the true $\beta_{\op{true}}$ coefficient which could be significant compared to
 $\beta_{\op{true}}$. Notice that in this case $XX^T$ is rank 1.
    
   \item  What does the corresponding estimate of the response $y_{\op{OLS}} := X^T\beta_{\op{OLS}}$ equal to when $\alpha \rightarrow 1$? Is it collinear with the true feature $w_1$ when $\alpha \rightarrow 1$? Explain what is happening.\\
   
  \medskip
  Taking $\alpha \rightarrow 1$, $w_2 \rightarrow w_1$ and we have 
  \begin{align*}
	  y_{\op{OLS}} := X^T\beta_{\op{OLS}}	&\rightarrow [w_1 \; w_2]  \MAT{\beta_{\op{true}} + 0.05  \\ 0.05} \\
	  								&\rightarrow [w_1 \; w_1] \MAT{\beta_{\op{true}} + 0.05  \\ 0.05} \\
									&\rightarrow ( \beta_{\op{true}} + 0.1) w_1
  \end{align*}
  When $\alpha \rightarrow 1$, the correlated feature $w_2 \rightarrow w_1$, and the response variable is collinear with the true feature $w_1$, the OLS linear model ignores $w_2$ and estimates $y_{\op{OLS}}$ as a linear scaling of $w_1$ 
  up to a factor of $0.1$ which could be an important error depending the magnitude of the linear coefficient  $\beta_{\op{true}}$ compared to 0.1.
  
  \item What does the ridge regression estimator of the coefficients $\beta_{\op{RR}}$ equal to when $\alpha \rightarrow 1$ and the regularization parameter $\lambda >0$ is fixed? Describe the difference with the OLS estimate.\\
 
  \medskip
  By definition the ridge regression estimator of the coefficients $\beta_{\op{RR}}$ is:
 \begin{align*}
 	\beta_{\op{RR}}				&=	(X X^T + \lambda I)^{-1}	X y	\; \lambda > 0	\\
	X X^T + \lambda I			&=	\MAT{1+ \lambda & \alpha \\ \alpha  &1+ \lambda} \\
	(X X^T + \lambda I)^{-1}		&=	\frac{1}{(1 + \lambda)^2 - \alpha^2} \MAT{1+ \lambda & -\alpha \\ -\alpha & 1+ \lambda} \\	
	\Rightarrow \beta_{\op{RR}}	&=	\frac{1}{(1 + \lambda)^2 - \alpha^2} \MAT{1+ \lambda & -\alpha \\ -\alpha & 1+ \lambda} \MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1} \\
							&=	\frac{1}{(1 + \lambda)^2 - \alpha^2}  \MAT{(1+ \lambda) (\beta_{\op{true}} + 0.1) -\alpha^2  \beta_{\op{true}} -0.1 \alpha \\
																-\alpha \beta_{\op{true}}  -0.1 \alpha + \alpha (1+\lambda) \beta_{\op{true}}  + 0.1 (1 + \lambda) } \\
							&=	\frac{1}{(1 + \lambda)^2 - \alpha^2}  \MAT{(1+ \lambda  -\alpha^2) \beta_{\op{true}}  + 0.1 (1+ \lambda  -\alpha) \\ 			
																\alpha \lambda \beta_{\op{true}} + 0.1 (1+ \lambda  -\alpha)} \\
							&=	 \MAT{\frac{(1+ \lambda  -\alpha^2) \beta_{\op{true}}} {(1 + \lambda)^2 - \alpha^2} + \frac{0.1} {1 + \lambda + \alpha} \\
									\frac{ \lambda \alpha  \beta_{\op{true}}} {(1 + \lambda)^2 - \alpha^2} + \frac{0.1} {1 + \lambda + \alpha} } \\
							&_{\alpha \rightarrow 1}\rightarrow \frac{ \beta_{\op{true}} + 0.1}{ 2 + \lambda} \MAT{1  \\ 1} 
 \end{align*} 
If we compare to $\beta_{\op{OLS}}$ estimator,  the ridge regression estimator has coefficients on both $w_1$ and $w_2$ and this coefficient can be regularized using $\lambda$. Even if $XX^T$ is singular (rank $1$), the l$_2$ regularization 
using $\lambda$ makes the matrix $XX^T$ non singular and we are still able to find a solution which includes both features.
 
  \item  What does the corresponding estimate of the response $y_{\op{RR}} := X^T\beta_{\op{RR}}$ equal to when $\alpha \rightarrow 1$? Is it collinear with the true feature $w_1$?
  \end{enumerate} 
  
 \item (Prior knowledge) Consider a linear regression problem where we have prior information indicating that the coefficients should be close to a certain value $\beta_{\op{prior}}$. 
 \begin{enumerate}
   \item How can you incorporate this prior knowledge if you are using ridge regression? Write the corresponding optimization problem. 
   \item Assume that the data are generated according to a linear model $\rnd{y}:= X^T \beta_{\op{true}} + \rnd{z}$, where $\beta_{true}\in \R^{p}$ and $X  \in \R^{p \times n}$ are fixed and $\rnd{z}$ is an iid Gaussian random vector with zero mean and variance $\sigma^2$. Does the modification change the mean or the covariance matrix of the estimator? If so, report the new value.
    \item How can you incorporate this prior knowledge if you are using gradient descent with early stopping? Write the corresponding update equation as a function of $\beta_{\op{prior}}$.
    \item Assume that the data are generated according to the linear model described above. Does the modification change the mean or the covariance matrix of the estimator? If so, report the new value.
 \end{enumerate}
 
\item The code you will implement in this question is located in the
  \verb|regress.py| file in the time folder of  \verb|hw5.zip|.
  Define a sequence of random variables as follows:
  \begin{align*}
    \rvx [0] &= 1\\
    \rvx[1] &= \rvx[0]+\rvz[1]\\
    \rvx[2] &= \rvx[1]+\rvz[2]\\
    \rvx[3] &= \rvx[2]+\rvz[3]\\
    \rvx[4] &= \rvx[3]+\rvz[4],
  \end{align*}
  where $\rvz[1],\rvz[2],\rvz[3],\rvz[4]$ are independent,
  $\rvz[1]\sim\Norm(0,1)$ and
  $\rvz[2],\rvz[3],\rvz[4]\sim\Norm(0,0.01^2)$.  There is a function
  $f:\RR^5\to\RR$ of the form $f(x)=\vbeta^Tx$ where $\vbeta$ is
  unknown.  We are given a training sample of independent draws
  $$(\rvx_1,f(\rvx_1)+\rw_1),\ldots,(\rvx_n,f(\rvx_n)+\rw_n)\in\RR^5\times\RR,$$
  where $\rw_i$ are iid standard normal random variables corrupting
  our measurements of $f$.
  Using this training data, we will estimate $\vbeta$ and test our
  performance on a validation set drawn from the same distribution.
  Below we refer to the square loss function
  $L:\RR^5\times\RR^{n\times 5}\times\RR^n\to\RR$ defined by
  $$L(\hat{\beta},X,y) = \sum_{i=1}^n (X[i,:]\hat{\beta}-y[i])^2$$
  where $X\in\RR^{n\times 5}$ denotes a matrix of data (training or validation; each row
  is a data point), and $y$ is the corresponding vector of $f$-values.
  \begin{enumerate}
  \item Using least squares (i.e., minimizing the square loss on the
    training set) compute an estimate for $\vbeta$.  Include your
    estimate for $\vbeta$, your square loss on the training set, and
    your square loss on the validation set in your submission.
    [Hint: If computed correctly your training loss should be larger
      than 30 and your validation loss should be larger than 10.]
  \item Compute the singular values of the training data matrix
    $X\in\RR^{n\times 5}$.
  \item The true value of $\vbeta$ can be found at the top of
    \verb|regress.py|.  Give an explanation as to why the least squares
    estimates aren't close to the true $\vbeta$-values.
  \item Use ridge regression to produce a new estimate of $\vbeta$
    and report the resulting estimate of $\vbeta$, and
    your square loss on the training and validation sets.  Here
    $\hat{\beta}$ should solve
    $$\text{minimize}_{\veta}\quad \|X\veta-\vy\|_2^2 + 0.5\|\veta\|_2^2.$$
  \end{enumerate}
You're not required to include your code in your submission, but you are free to do so. 
 \end{enumerate}
\end{document}
