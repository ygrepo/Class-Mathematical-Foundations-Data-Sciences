\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}

\input{macros}
%\input{Symbols}

\begin{document}

\noindent DS-GA.1013 Mathematical Tools for Data Science \\
Homework 5\\
Yves Greatti - yg390\\

\begin{enumerate}

\item (Augmented dataset) Ridge regression is equivalent to applying OLS on an expanded dataset that has additional examples. Describe these additional examples in detail. Intuitively, what effect do these additional examples have?\\
\medskip

The ridge regression estimate is defined as $\beta_{\op{RR}} = (X X^T + \lambda I)^{-1} X y$, where $X \in \R^{p \times n}, y \in \R^n, \beta \in \R^p$, which can be reformulated as a modified least-square problem
$$\beta_{\op{RR}} = \argmin_\beta \big\| \begin{bmatrix} y   \\ 0 \end{bmatrix} - \begin{bmatrix} X^T   \\ \sqrt{\lambda} I_{n \times p} \end{bmatrix} \beta \big\|_2^2$$
It seems like we have added $p$ vectors to the original $n$ datapoints $[x_i, \ldots, x_n], x_i \in \R^p, i=1, \ldots, n$. By doing so, Ridge regression embedded the original problem in $\R^n$ into a larger space $\R^{n+p}$ by moving into $p$ different 
directions with a small amount $\sqrt{\lambda}$ which could decrease any collinearity present in the original data points $X$. As we have seen in the notes of linear regression, when the number of training data is small, $\lambda$ neutralizes the large variance of the errors between the true  $\beta$ coefficients and the ridge regression coefficients due to small singular values  of the sample covariance matrix.
 
 \newpage
 
\item  (Correlated features) Consider a regression problem where the response only depends on one feature, but we don't know it, so we incorporate an additional feature into the model that happens to be very correlated with the first feature. More specifically, let $y \in \R^{n}$ be defined by
\begin{align}
y := \beta_{\op{true}} w_1 + z, 
\end{align}
where $\beta_{\op{true}} \in \R$ is the true coefficient, $w_1 \in \R^n$ is the first feature vector, and $z \in \R^{n}$ is additive noise. The second feature vector is given by $w_2\in \R^n$ and can be decomposed into
\begin{align}
w_2 = \alpha w_1 + \sqrt{1-\alpha^2} w_{\perp},
\end{align}  
where $w_{\perp}$ is orthogonal to $w_1$. The vectors $w_1$, $w_2$, $w_{\perp}$ and $z$ all have unit $\ell_2$ norm. In addition, we assume
\begin{align}
w_1^Tz = 0.1, \\
w_{\perp}^Tz = 0.1.
\end{align}
We fit a linear regression model to $y$ using the feature matrix 
  \begin{align}
  X & = \MAT{w_1^T \\ w_2^T}.
  \end{align} 
 \begin{enumerate}
  \item What does the OLS estimator of the coefficients $\beta_{\op{OLS}}$ equal to when $\alpha \rightarrow 1$? Explain what is happening. \\
\emph{Hint:} Use the fact that for any $a$, $b$, $c$, and $d$ such that $ad \neq bc$
\begin{align}
 \MAT{a & b \\ c & d}^{-1} = \frac{1}{ad-bc} \MAT{d & -b \\ -c & a}.
\end{align}\\

\medskip

The OLS estimator of the coefficients of $\beta_{\op{OLS}}$  is given by $\beta_{\op{OLS}} = (X X^T)^{-1} X y$.
Expanding each term, we have:
\begin{align*}
 	(X X^T)				&=	\MAT{w_1^T \\ w_2^T} [w_1 \; w_2]	\\
						&=	\MAT{w_1^T w_1 & w_1^T w_2 \\ w_2^T w_1 & w_2^T w_2}	\\
						&= 	\MAT{\|w_1\|_2^2 & w_1^T w_2 \\ w_2^T w_1  & \|w_2\|_2^2} \\
	w_1^T w_2			&=	w_1^T (\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp}) \\
						&=	\alpha \|w_1\|_2^2 + \sqrt{1-\alpha^2}  w_1^T w_{\perp} \\
						&=	\alpha \text{ ~ since } \|w_1\|_2 = 1 \;, w_1 \perp  w_{\perp} \\
	w_2^T w_2			&= 	(\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp})^T (\alpha w_1 + \sqrt{1-\alpha^2} w_{\perp}) \\
						&= 	\alpha^2 w_1^T w_1 + 2 \alpha \sqrt{1-\alpha^2} w_1^T w_{\perp} + (1-\alpha^2) w_{\perp}^T w_{\perp} \\
						&=	\alpha^2 \| w_1 \|_2^2 +  (1-\alpha^2) \|w_{\perp}\|_2^2 \text{ ~  knowing that }  w_{\perp} \perp w_1 \\
						&=	\alpha^2 +  (1-\alpha^2) = 1 \; \text{by assumptions } \| w_1 \|_2 = \|w_{\perp}\|_2 = 1 \\
	\Rightarrow (X X^T)		&= 	\MAT{1 & \alpha \\ \alpha & 1} \\
	\Rightarrow (X X^T)^{-1}	&=	\frac{1}{1-\alpha^2} \MAT{1 & -\alpha \\ -\alpha  & 1} \\
	X y					&= 	\MAT{w_1^T \\ w_2^T} (\beta_{\op{true}} w_1 + z) \\
						&=	\MAT{\beta_{\op{true}} w_1^T w_1 + w_1^T z \\ \beta_{\op{true}} w_2^T w_1 + w_2^T z } \\
						&=	\MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1}	
\end{align*}
Substituting each of the previous terms back into the expression of $\beta_{\op{OLS}}$, we find that:
\begin{align*}
	\beta_{\op{OLS}}		&=	\frac{1}{1-\alpha^2} \MAT{1 & -\alpha \\ -\alpha  & 1}  \MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1} \\
						&= 	\frac{1}{1-\alpha^2}  \MAT{\beta_{\op{true}} + 0.1 -\alpha^2 \beta_{\op{true}} -0.1 \alpha \\ -\alpha \beta_{\op{true}} -0.1 \alpha + \alpha \beta_{\op{true}}  + 0.1} \\
						&=	\frac{1}{1-\alpha^2}  \MAT{(1-\alpha^2)  \beta_{\op{true}} + 0.1 (1-\alpha) \\ 0.1 (1 - \alpha)} \\
						&=  	\MAT{\beta_{\op{true}} + \frac{0.1} {1 + \alpha} \\ \frac{0.1} {1 + \alpha}}
\end{align*}
When $\alpha \rightarrow 1$ , $\beta_{\op{OLS}} \rightarrow   \MAT{\beta_{\op{true}} + 0.05  \\ 0.05}$.
The OLS estimator ignores the correlated feature $w_2$ and adds a fixed bias $0.05$ to the true $\beta_{\op{true}}$ coefficient which could be significant compared to
 $\beta_{\op{true}}$. Notice that in this case $XX^T$ is rank 1, and the algorithm used to find the OLS estimator may be unable to find a solution.
    
   \item  What does the corresponding estimate of the response $y_{\op{OLS}} := X^T\beta_{\op{OLS}}$ equal to when $\alpha \rightarrow 1$? Is it collinear with the true feature $w_1$ when $\alpha \rightarrow 1$? Explain what is happening.\\
   
  \medskip
  Taking $\alpha \rightarrow 1$, $w_2 \rightarrow w_1$ and we have 
  \begin{align*}
	  y_{\op{OLS}} := X^T\beta_{\op{OLS}}	&\rightarrow [w_1 \; w_2]  \MAT{\beta_{\op{true}} + 0.05  \\ 0.05} \\
	  								&\rightarrow [w_1 \; w_1] \MAT{\beta_{\op{true}} + 0.05  \\ 0.05} \\
									&\rightarrow ( \beta_{\op{true}} + 0.1) w_1
  \end{align*}
  When $\alpha \rightarrow 1$, the correlated feature $w_2 \rightarrow w_1$, and the response variable is collinear with the true feature $w_1$, the OLS linear model ignores $w_2$ and estimates $y_{\op{OLS}}$ as a linear scaling of $w_1$ 
  up to a factor of $0.1$ which could lead to an important error depending the magnitude of the linear coefficient  $\beta_{\op{true}}$ compared to 0.1.
    
  \item What does the ridge regression estimator of the coefficients $\beta_{\op{RR}}$ equal to when $\alpha \rightarrow 1$ and the regularization parameter $\lambda >0$ is fixed? Describe the difference with the OLS estimate.\\
 
  \medskip
  By definition the ridge regression estimator of the coefficients $\beta_{\op{RR}}$ is:
 \begin{align*}
 	\beta_{\op{RR}}				&=	(X X^T + \lambda I)^{-1}	X y	\; \lambda > 0	\\
	X X^T + \lambda I			&=	\MAT{1+ \lambda & \alpha \\ \alpha  &1+ \lambda} \\
	(X X^T + \lambda I)^{-1}		&=	\frac{1}{(1 + \lambda)^2 - \alpha^2} \MAT{1+ \lambda & -\alpha \\ -\alpha & 1+ \lambda} \\	
	\Rightarrow \beta_{\op{RR}}	&=	\frac{1}{(1 + \lambda)^2 - \alpha^2} \MAT{1+ \lambda & -\alpha \\ -\alpha & 1+ \lambda} \MAT{\beta_{\op{true}} + 0.1 \\ \alpha \beta_{\op{true}} + 0.1} \\
							&=	\frac{1}{(1 + \lambda)^2 - \alpha^2}  \MAT{(1+ \lambda) (\beta_{\op{true}} + 0.1) -\alpha^2  \beta_{\op{true}} -0.1 \alpha \\
																-\alpha \beta_{\op{true}}  -0.1 \alpha + \alpha (1+\lambda) \beta_{\op{true}}  + 0.1 (1 + \lambda) } \\
							&=	\frac{1}{(1 + \lambda)^2 - \alpha^2}  \MAT{(1+ \lambda  -\alpha^2) \beta_{\op{true}}  + 0.1 (1+ \lambda  -\alpha) \\ 			
																\alpha \lambda \beta_{\op{true}} + 0.1 (1+ \lambda  -\alpha)} \\
							&=	 \MAT{\frac{(1+ \lambda  -\alpha^2) \beta_{\op{true}}} {(1 + \lambda)^2 - \alpha^2} + \frac{0.1} {1 + \lambda + \alpha} \\
									\frac{ \lambda \alpha  \beta_{\op{true}}} {(1 + \lambda)^2 - \alpha^2} + \frac{0.1} {1 + \lambda + \alpha} } \\
							&_{\alpha \rightarrow 1}\rightarrow \frac{ \beta_{\op{true}} + 0.1}{ 2 + \lambda} \MAT{1  \\ 1} 
 \end{align*} 
If we compare to $\beta_{\op{OLS}}$ estimator,  the ridge regression estimator has coefficients on both $w_1$ and $w_2$ and this coefficient can be regularized using $\lambda$. Even if $XX^T$ is singular (rank $1$), the l$_2$ regularization 
using $\lambda$ makes the matrix $XX^T$ non singular and we are still able to find a solution which includes both features. In addition we have a new set of coefficients each time we tune $\lambda$:
when $\lambda \rightarrow 0, \beta_{\op{RR}} = \frac{\beta_{\op{OLS}}} {2}$ and when $\lambda \rightarrow \infty, \beta_{\op{RR}} \rightarrow 0$. So the ridge regression estimator with the regularization parameter, $\lambda$,
governs the trade-off between a model that has good fit on the training set (low bias) and a model which reduces the test error (low variance). 
 
  \item  What does the corresponding estimate of the response $y_{\op{RR}} := X^T\beta_{\op{RR}}$ equal to when $\alpha \rightarrow 1$? Is it collinear with the true feature $w_1$?\\
  \medskip
  When  $\alpha \rightarrow 1, w_2 \rightarrow w_1$, which leads to
 \begin{align*}
 	y_{\op{RR}} := X^T\beta_{\op{RR}}	&\rightarrow	\frac{ \beta_{\op{true}} + 0.1}{ 2 + \lambda} [w_1 w_2]  \MAT{1  \\ 1} \\
								&\rightarrow 	\frac{ \beta_{\op{true}} + 0.1}{ 2 + \lambda} [w_1 w_1] \MAT{1  \\ 1} \\
								&\rightarrow 	2 \;  \frac{\beta_{\op{true}} + 0.1}{ 2 + \lambda} \; w_1
 \end{align*}    
  $y_{\op{RR}}$ is collinear with the true feature $w_1$, compared to $y_{\op{OLS}}$, the parameter $\lambda$ allows to control the amount of linearity between the response and control variable (when $\lambda=0, y_{\op{RR}} = y_{\op{OLS}}$), which might be desirable if the test data points, not known in advance, are not totally dependent on the feature $w_1$.
  
  \end{enumerate} 
 
 \newpage
 
 \item (Prior knowledge) Consider a linear regression problem where we have prior information indicating that the coefficients should be close to a certain value $\beta_{\op{prior}}$. 
 \begin{enumerate}
 
   \item How can you incorporate this prior knowledge if you are using ridge regression? Write the corresponding optimization problem.\\
 
  \medskip
   If we want to include that the coefficients should be close to a  $\beta_{\op{prior}}$, the ridge-regression estimator is the minimizer of the optimization problem:
   $$\beta_{\op{RR}} := \argmin_\beta \| y - X^T \beta \|_2^2 + \lambda \| \beta - \beta_{\op{prior}} \|_2^2$$
   where $\lambda > 0$ is a fixed regularization parameter.
   
   \item Assume that the data are generated according to a linear model $\rnd{y}:= X^T \beta_{\op{true}} + \rnd{z}$, where $\beta_{true}\in \R^{p}$ and $X  \in \R^{p \times n}$ are fixed and $\rnd{z}$ is an iid Gaussian random vector with zero mean and variance $\sigma^2$. Does the modification change the mean or the covariance matrix of the estimator? If so, report the new value.\\
   
\medskip
 
 \begin{align*}
 	\| y - X^T \beta \|_2^2		&=	(y - X^T \beta)^T (y - X^T \beta) + \lambda (\beta - \beta_{\op{prior}})^T (\beta - \beta_{\op{prior}}) \\
							&=	(y^T - \beta^T X)  (y - X^T \beta) + \lambda (\beta^T - \beta_{\op{prior}}^T) (\beta - \beta_{\op{prior}}) \\
							&=	\beta^T X X^T \beta -2 (X y)^T \beta + y^T y + \lambda \big( \beta^T \beta -2  \beta_{\op{prior}}^T \beta +  \beta_{\op{prior}}^T \beta_{\op{prior}} \big) \\
							&=	\beta^T X X^T \beta -2 (X y)^T \beta + \lambda \big( \beta^T \beta -2  \beta_{\op{prior}}^T \beta) +  y^T y +  \lambda  \beta_{\op{prior}}^T \beta_{\op{prior}} := f(\beta)
 \end{align*}    
 $f$ is a quadratic form in $\beta$ and its gradient and Hessian equal:
 \begin{align*}
 	\nabla_{\beta} f(\beta)		&=	2 (X X^T \beta - X y +  \lambda (\beta - \beta_{\op{prior}}))  \\
	\nabla_{\beta}^2 f(\beta)		&=	2 (X X^T  +  \lambda I) 
 \end{align*}    
  $v^T (X X^T  +  \lambda I) v  = v^T X X^T v + \lambda v^T v = \|X^T v \|_2^2 + \lambda \| v \|_2^2 \ge 0$ and it is zero only when $v$ is the zero vector (by property of the norm operator), thus the matrix $X X^T  +  \lambda I$ is positive definite
  and invertible. The unique minimum can be found by setting the gradient to zero, which gives that the ridge regression estimator is:
  
  $$\beta_{\op{RR}}  =   (X X^T  +  \lambda I)^{-1} (X y + \lambda \beta_{\op{prior}})$$
  with mean:
\begin{align*}
  	\E[\beta_{\op{RR}} ]		&= 		\E[ (X X^T  +  \lambda I)^{-1} (X X^T \beta_{\op{true}} + X \rnd{z} +  \lambda \beta_{\op{prior}})] \\
						&=		 (X X^T  +  \lambda I)^{-1} (X X^T \beta_{\op{true}}  + \lambda \beta_{\op{prior}}) \\
						&	\text{ ~ by linearity of the expectation and } \rnd{z} \text{ has zero mean}
\end{align*} 
Let $X=U S V^T$ the SVD of X, we can then expand the previous expression:
\begin{align*}
	\E[\beta_{\op{RR}} ]		&=	(U S^2 U^T + \lambda U \; U^T)^{-1}	(U S^2 U^T \beta_{\op{true}}  + \lambda \beta_{\op{prior}}) \\
						&=	U (S^2 + \lambda I)^{-1} S^2 U^T  \beta_{\op{true}}  +  \lambda \beta_{\op{prior}} U (S^2 + \lambda I)^{-1} U^T
\end{align*} 

and variance:	
\begin{align*}
	\Var(\beta_{\op{RR}})	&=	\Var( (X X^T  +  \lambda I)^{-1} (X y + \lambda \beta_{\op{prior}})) \\
						&=	\Var( (X X^T  +  \lambda I)^{-1}  X y + \lambda  \beta_{\op{prior}} (X X^T  +  \lambda I)^{-1} ) \\
						&=	\Var( (X X^T  +  \lambda I)^{-1}  X y) \\
						&=	(X X^T  +  \lambda I)^{-1}  X \var(y)	((X X^T  +  \lambda I)^{-1}  X)^T \\
						&=	\sigma^2 (X X^T  +  \lambda I)^{-1}  X X^T (X X^T  +  \lambda I)^{-1} \\
						&=	\sigma^2 U (S^2 + \lambda I)^{-1} S^2 (S^2 + \lambda I)^{-1} U^T		
\end{align*}					
   
    \item How can you incorporate this prior knowledge if you are using gradient descent with early stopping? Write the corresponding update equation as a function of $\beta_{\op{prior}}$.\\
\medskip

For $X  \in \R^{p \times n}$ and a response vector $y \in \R^n$, from part (a) we know that the gradient equals $$\nabla_{\beta} f(\beta) =	2 (X X^T \beta - X y +  \lambda (\beta - \beta_{\op{prior}}))$$
The gradient-descent updates are:
\begin{align*}
		\beta^{(k+1)}	&=	\beta^{(k)} - \; \alpha_k \nabla_{\beta} f(\beta^{(k)})	\\
					&=	\beta^{(k)}  - \alpha_k (X X^T \beta^{(k)} - X y +  \lambda (\beta^{(k)} - \beta_{\op{prior}})) \text{ ~ absorbing the constant } 2 \text{ in the } \alpha _k \text{ parameter } \\
					&=    ((1- \lambda \alpha_k) I  - \alpha_k X X^T) \beta^{(k)}  +  \alpha_k  X y  + \lambda  \alpha_k  \beta_{\op{prior}} \\
					&=	 \beta^{(k)}  +  \alpha_k  \big( \sum_{i=1}^n (y[i] - \PROD{x_i}{\beta^{(k)}}) x_i - \lambda ( \beta^{(k)}  -   \beta_{\op{prior}}) \big)
\end{align*}
   where $\beta^{(k)} \in \R^p$ and $ \alpha_k > 0$ which are the estimated step size at iteration $k$. Note that contrary to the linear regression update at step $k$, we now have an extra term 
   which includes the prior $\beta_{\op{true}}$ scaled by the regularization parameter $\lambda$.
    
    \item Assume that the data are generated according to the linear model described above. Does the modification change the mean or the covariance matrix of the estimator? If so, report the new value.\\
    
\medskip

Expressing the previous expression as:
\begin{align*}
		\beta^{(k+1)}	&=	\big( (1 - \lambda \alpha)  I - \alpha X X^T \big) \beta^{(k)}		+  	\alpha (X y + \lambda \beta_{\op{prior}}) \\
					&=	\big(I - \alpha (\lambda I + X X^T)  \big)^{k+1} \beta^{(0)}		+	\sum_{i=0}^k \big(I - \alpha (\lambda I + X X^T)  \big)^i \alpha (X y + \lambda \beta_{\op{prior}} I )
\end{align*}

Assuming $X$ full rank and p $\le$ n, $UU^T = U^TU = I$, let $X = USV^T$, we have then:
\begin{align*}
		\beta^{(k+1)}	&=	U (I - \alpha (S^2 + \lambda I))^{k+1} U^T \beta^{(0)}		+ \alpha \sum_{i=0}^k 	U (I - \alpha (S^2 + \lambda I))^ i U^T (U S V^T y +  \lambda \beta_{\op{prior}} I) \\
					&=	U (I - \alpha (S^2 + \lambda I))^{k+1} U^T \beta^{(0)}		+ \alpha \sum_{i=0}^k 	U (I - \alpha (S^2 + \lambda I))^ i (S V^T y + \lambda \beta_{\op{prior}}  U^T) \\
					&=	U \text{diag}_{j=1}^p \bigg( (1 - \alpha (s_j^2 + \lambda))^{k+1} \bigg)U^T \beta^{(0)}	\\
					&+ \alpha \; U  \;  \text{diag}_{j=1}^p \bigg( \sum_{i=0}^k (1 - \alpha (s_j^2 + \lambda))^i \bigg)  (S V^T y + \lambda \beta_{\op{prior}}  U^T) \\
					&= U \text{diag}_{j=1}^p \bigg( (1 - \alpha (s_j^2 + \lambda))^{k+1} \bigg)U^T \beta^{(0)}	\\
					&+ U  \;  \text{diag}_{j=1}^p \bigg( \frac{ 1 - (1 - \alpha (s_j^2 + \lambda))^{k+1}} {s_j^2 + \lambda} \bigg)  (S V^T y + \lambda \beta_{\op{prior}}  U^T)
\end{align*}
If step size $\alpha$ is small enough $0 < \alpha < \frac{2} {\lambda + s_j^2} \rightarrow |1 - (1 - \alpha (s_j^2 + \lambda)| < 1 \rightarrow \lim_{k \rightarrow \infty} (1 - (1 - \alpha (s_j^2 + \lambda))^k =0, j=1, \dots,p $ then gradient descent converges to:
$$\lim_{k \rightarrow \infty} \beta^{(k+1)} = U  \;  \text{diag}_{j=1}^p \bigg( \frac{1} {s_j^2 + \lambda} \bigg)  (S V^T y + \lambda \beta_{\op{prior}}  U^T)$$

%\begin{align*}
%	\lim_{k \rightarrow \infty} \beta^{(k)}	&=  U  \;  \text{diag}_{j=1}^p \bigg(  \frac{s_j} {s_j^2 + \lambda}  \bigg)  (V^T y + \lambda \beta_{\op{prior}}  U^T) \\
%								&=  U  \;  \text{diag}_{j=1}^p \bigg(  \frac{s_j} {s_j^2 + \lambda}  \bigg)  (V^T V S U^T \beta_{\op{true}} + V^T \rnd{z} +   \lambda \beta_{\op{prior}}  U^T) \\
%								&=  U \;    \text{diag}_{j=1}^p \bigg(  \frac{s_j^2} {s_j^2 + \lambda}  \bigg) U^T  \beta_{\op{true}} + U  \lambda \beta_{\op{prior}}  U^T \\
%								&+  U \;    \text{diag}_{j=1}^p \bigg(  \frac{s_j} {s_j^2 + \lambda}  \bigg) V^T  \rnd{z}
%\end{align*}	

Let $\nu_j := 1 - \alpha (s_j^2 + \lambda)$, then we now have
\begin{align*}
	\rnd{\beta}^{(k)} 	&= 	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{ 1 - \nu_j^k} {s_j^2 + \lambda}  \bigg) (S V^T X^T  \beta_{\op{true}}  + S V^T  \rnd{z} +  \lambda \beta_{\op{prior}}  U^T)\\
					&=	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{ 1 - \nu_j^k} {s_j^2 + \lambda}  \bigg) (S^2 U^T  \beta_{\op{true}}  +  \lambda \beta_{\op{prior}}  U^T  + S V^T  \rnd{z}) \\	
					&=	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{s_j^2  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) U^T \beta_{\op{true}}  + \lambda U \;    \text{diag}_{j=1}^p \bigg(  \frac{1 - \nu_j^k} {s_j^2 + \lambda} \bigg) \beta_{\op{prior}} U^T \\
					&+  	 U \;    \text{diag}_{j=1}^p \bigg(  \frac{ s_j  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) V^T  \rnd{z} 
\end{align*}	
 Then using Theorem 8.6 in the PCA lecture notes, we have the mean:
\begin{align*}
	\beta_{\text{bias}} 	&= \sum_{i=1}^p \frac{1 - \nu_j^k}{s_i^2 + \lambda} \PROD{u_i} {s_i^2 \beta_{\op{true}}+ \lambda  \beta_{\op{prior}} } u_i \\
					&= \sum_{i=1}^p \frac{1 - (1 - \alpha (s_i^2 + \lambda))^k} {s_i^2 + \lambda} \PROD{u_i} {s_i^2 \beta_{\op{true}}+ \lambda  \beta_{\op{prior}} } u_i \
\end{align*}
The variance is 
\begin{align*}
	\Sigma			&= \sigma^2 U  \text{diag}_{j=1}^p \bigg(  \frac{ s_j  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) V^T V  \text{diag}_{j=1}^p \bigg(  \frac{ s_j  (1 - \nu_j^k)} {s_j^2 + \lambda} \bigg) U^T \\
					&= \sigma^2 U  \text{diag}_{j=1}^p \bigg(  \frac{ s_j^2  (1 - (1 - \alpha (s_i^2 + \lambda))^k)} {(s_j^2 + \lambda)^2} \bigg) U
\end{align*}

 
 \end{enumerate}
 
\item The code you will implement in this question is located in the
  \verb|regress.py| file in the time folder of  \verb|hw5.zip|.
  Define a sequence of random variables as follows:
  \begin{align*}
    \rvx [0] &= 1\\
    \rvx[1] &= \rvx[0]+\rvz[1]\\
    \rvx[2] &= \rvx[1]+\rvz[2]\\
    \rvx[3] &= \rvx[2]+\rvz[3]\\
    \rvx[4] &= \rvx[3]+\rvz[4],
  \end{align*}
  where $\rvz[1],\rvz[2],\rvz[3],\rvz[4]$ are independent,
  $\rvz[1]\sim\Norm(0,1)$ and
  $\rvz[2],\rvz[3],\rvz[4]\sim\Norm(0,0.01^2)$.  There is a function
  $f:\RR^5\to\RR$ of the form $f(x)=\vbeta^Tx$ where $\vbeta$ is
  unknown.  We are given a training sample of independent draws
  $$(\rvx_1,f(\rvx_1)+\rw_1),\ldots,(\rvx_n,f(\rvx_n)+\rw_n)\in\RR^5\times\RR,$$
  where $\rw_i$ are iid standard normal random variables corrupting
  our measurements of $f$.
  Using this training data, we will estimate $\vbeta$ and test our
  performance on a validation set drawn from the same distribution.
  Below we refer to the square loss function
  $L:\RR^5\times\RR^{n\times 5}\times\RR^n\to\RR$ defined by
  $$L(\hat{\beta},X,y) = \sum_{i=1}^n (X[i,:]\hat{\beta}-y[i])^2$$
  where $X\in\RR^{n\times 5}$ denotes a matrix of data (training or validation; each row
  is a data point), and $y$ is the corresponding vector of $f$-values.
  \begin{enumerate}
  \item Using least squares (i.e., minimizing the square loss on the
    training set) compute an estimate for $\vbeta$.  Include your
    estimate for $\vbeta$, your square loss on the training set, and
    your square loss on the validation set in your submission.
    [Hint: If computed correctly your training loss should be larger
      than 30 and your validation loss should be larger than 10.]
  \item Compute the singular values of the training data matrix
    $X\in\RR^{n\times 5}$.
  \item The true value of $\vbeta$ can be found at the top of
    \verb|regress.py|.  Give an explanation as to why the least squares
    estimates aren't close to the true $\vbeta$-values.
  \item Use ridge regression to produce a new estimate of $\vbeta$
    and report the resulting estimate of $\vbeta$, and
    your square loss on the training and validation sets.  Here
    $\hat{\beta}$ should solve
    $$\text{minimize}_{\veta}\quad \|X\veta-\vy\|_2^2 + 0.5\|\veta\|_2^2.$$
  \end{enumerate}
You're not required to include your code in your submission, but you are free to do so. 
 \end{enumerate}
\end{document}
