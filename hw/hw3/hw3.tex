\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage[psamsfonts]{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}

\input{macros}
\begin{document}

\noindent DS-GA.1013 Mathematical Tools for Data Science \\
Homework 3 \\
Yves Greatti - yg390\\

\begin{enumerate}
\item (PCA and linear regression) Consider a dataset of $n$ 2-dimensional data points $x_1,\ldots,x_n \in \R^2$. Assume that the dataset is centered. Our goal is to find a line in the 2D space that lies \emph{closest} to the data. First, we apply PCA and consider the line in the direction of the first principal direction. Second, we fit a linear regression model where $x_i[1]$ is a feature, and $x_i[2]$ the corresponding response. Are these lines the same? Describe each line in terms of the quantity it minimizes geometrically (e.g. sum of some distance from the points to the lines).

 \item (Heartbeat) We are interested in computing the best linear estimate of the heartbeat of a fetus in the presence of strong interference in the form of the heartbeat of the baby's mother. To simplify matters, let us assume that we only want to estimate the heartbeat at a certain moment. We have available a measurement from a microphone situated near the mother's belly and another from a microphone that is away from her belly. We model the measurements as
\begin{align}
\rx[1] & = \rb + \rnd{m} + \rnd{z_1}\\
\rx[2] & = \rnd{m} + \rnd{z_2},
\end{align}
where $\rb$ is a random variable modeling the heartbeat of the baby, $\rnd{m}$ is a random variable modeling the heartbeat of the mother, and $\rnd{z}_1$ and $\rnd{z}_2$ model additive noise. From past data, we determine that $\rb$, $\rnd{m}$, $\rnd{z}_1$, and $\rnd{z}_2$ are all zero mean and uncorrelated with each other. The variances of $\rb$, $\rnd{z}_1$ and $\rnd{z}_2$ are equal to $1$, whereas the variance of $\rnd{m}$ is much larger, it is equal to $10$.
\begin{enumerate}
\item Compute the best linear estimator of $\rb$ given $\rx[1]$ in terms of MSE, and the corresponding MSE. Describe in words what the estimator does. 

We have shown in class that centering the variables does not change the MSE, so we want to estimate 
MSE = $\min_\beta \E[(\rb - \beta \rx[1])^2] = \beta^2 \var(\rx[1]) + \var(\rb) -2 \beta \cov(\rx[1], \rb)$.
\begin{align*}
	 \var(\rx[1])		&= 	\var(\rb + \rnd{m} + \rnd{z_1}) \\
	 				&= 	\var(\rb) + \var(\rnd{m} )	+ \var(\rnd{z_1})	\\
					&=	1 + 10 + 1	=12	 \\
	\cov(\rx[1], \rb)		&=	\E[\rx[1] \rb]	\\
					&= 	\E[ (\rb + \rnd{m} + \rnd{z_1}) \rb] = \E[\rb^2] = 1
\end{align*}
Since $\rb$, $\rnd{m}$, $\rnd{z}_1$ are all zero mean and uncorrelated with each other.

MSE = $12 \beta^2 -2 \beta + 1$, it is a convex quadratic function with respect to $\beta$, so we can set the derivative to zero to find the minimum: $\beta^* = \frac{1}{12}$.
MSE$_{\beta^*} = \frac{11}{12} = 0.91$. This estimator predicts the heartbeat of the baby using the measurement $\rx[1]$ from the microphone situated near  the mother's belly.

\item Compute the best linear estimator of $\rb$ given $\rx$ in terms of MSE, and the corresponding MSE. Describe in words what the estimator does. 
MSE of this estimator equals $\var(\rb) - \Sigma_{\rb\rx}^T \Sigma_{\rx}^{-1} \Sigma_{\rb\rx}$.
$$\cov(\rx[1], \rx[2]) = \E[\rx[1]  \rx[2]] = \E[\rx[1]] \E[\rx[2]] =   \E[\rx[1] \rx[2]]$$ Since $\E[\rx[1]] = \E[\rx[2]] = 0$ by linearity of the expectation.
And by assumptions 
\begin{align*}
	\cov(\rx[1], \rx[2]) 	&= 	\E[\rx[1]]  \E[\rx[2]] = \E[\rnd{m}^2] = \var(\rnd{m}) = 10	\\
	 \var(\rx[2])		&=	\var(\rnd{m} + \rnd{z_2}) = \var(\rnd{m}) + \var(\rnd{z_2}) 	\\
	 				&=	10 + 1 = 11	\\
	\cov(\rb, \rx)		&=	\E[\rb \rx] - \E[\rb] \E[\rx] = \E[\rb \rx] \\
					&= 	[\E[\rx[1] \rb] \; \E[\rx[2] \rb]]^T = [1 \; 0]^T
\end{align*}



This gives us:
\begin{align*}
	\Sigma_{\rx} &= 
	\begin{bmatrix}
		\var(\rx[1])	&			\cov(\rx[1], \rx[2]) \\
		\cov(\rx[2], \rx[1])	&	\var(\rx[2])		\\
	\end{bmatrix}
	=
	\begin{bmatrix}
		12	&	10 \\
		10	&	11 
	\end{bmatrix} \\
	\Sigma_{\rx}^{-1} &= 
	\begin{bmatrix}
		\frac{11}{32}	&	-\frac{5}{16} \\
		-\frac{5}{16}	&	-\frac{3}{8}
	\end{bmatrix} \\
	\Sigma_{\rb\rx}^T \Sigma_{\rx}^{-1} \Sigma_{\rb\rx} &=
	[1 \; 0] 	\begin{bmatrix} \frac{11}{32} \\ -\frac{5}{16} \end{bmatrix} = \frac{11}{32}
\end{align*}
Hence MSE = $ 1 - \frac{11}{32} = \frac{21}{32} = 0.65$. The second estimator provides a better estimation of the heartbeat of the baby by jointly using the two microphones.

\end{enumerate}

\item (Gaussian minimum MSE estimator) In this problem we derive the minimum MSE estimator of a random variable $\rnd{b}$ given another random variable $\rnd{a}$ when both are jointly Gaussian. To simplify matters we assume the mean of both random variables is zero. 
  \begin{enumerate}
  \item Let us define
  \begin{align}
  \rnd{c} := \frac{\cov(\ra,\rb)}{\var(\ra)}\ra.
  \end{align}
  Consider the decomposition of $\rnd{b}$ into the sum of $\rnd{c} $ and $\rb - \rnd{c}$. Provide a geometric interpretation of this decomposition. 
  This decomposition is the orthogonal projection of $\rnd{b}$ into a vector in the span of $\rnd{a}$: $\rnd{c}$ and a vector orthogonal to this hyperspace:  $\rb - \rnd{c}$.
  
  \item Compute the conditional expectation of $\rnd{c} $ given $\ra=a$ for a fixed $a \in \R$.
  $$\E[\rnd{c} | \rnd{a}= a] = \E[\frac{\cov(\ra,\rb)}{\var(\ra)}\ra | \ra= a] = \frac{\cov(\ra,\rb)}{\var(\ra)}\ra$$
  
  \item Compute the conditional expectation of $\rb - \rnd{c}$ given $\ra=a$ for a fixed $a \in \R$. (Hint: Start by computing the covariance between $\rb - \rnd{c}$ and $\ra$.)
  First few results
  \begin{align*}
  	\cov(\ra, \rb)	&=	 \E[\ra \; \rb]	- \E[\ra] \; \E[\rb]	\\
				&=	 \E[\ra \; \rb] - 0 = \E[\ra \; \rb] 		\\
	\var(\ra)		&=	 \E[\ra^2] - \E[\ra]^2 =  \E[\ra^2]		\\
	\E[\rb  - \rnd{c}]	&= 	 \E[ \rb  - \frac{\cov(\ra,\rb)}{\var(\ra)}\ra] \\
				&=	 \E[ \rb] - \frac{\cov(\ra,\rb)}{\var(\ra)} \E[\ra] = 0 
  \end{align*}
  Hence $$\cov(\rb  - \rnd{c}, \ra)   = \E[ \rb  \; \ra] -  \frac{\cov(\ra,\rb)}{\var(\ra)} \E[\ra^2]  = \cov(\ra, \rb) - \cov(\ra, \rb) = 0$$
  Thus  $\rb  - \rnd{c}$ and $\ra$ are uncorrelated and $\E[ \rb - \frac{\cov(\ra,\rb)}{\var(\ra)} \ra | \ra= a ] = \E[ \rb - \frac{\cov(\ra,\rb)}{\var(\ra)} \ra] = \E[ \rb] -  \frac{\cov(\ra,\rb)}{\var(\ra)}  \E[\ra] = 0$.
  
  \item Prove that the minimum MSE estimator of $\rnd{b}$ given $\ra=a$ for a fixed $a \in \R$ is linear. 
  
  Using the problem assumptions, and theorem 2.1 from our class, the minimum MSE estimator of $\rnd{b}$ given $\ra=a$ for a fixed $a \in \R$ is given by:
  \begin{align*}
  	\E[\rb | \ra = a]		&=	\E[\rb-  \rnd{c} +  \rnd{c} | \ra= a]	\\
					&=	\E[\rb-  \rnd{c} | \ra= a] + \E[\rnd{c} | \ra= a] \\
					&=	\frac{\cov(\ra,\rb)}{\var(\ra)} \ra
  \end{align*}
  
  
  \item What step of the proof fails for non-Gaussian random variables?
  Since $\ra$ and $\rb$ are gaussian random variables then $\ra$ and $\rb -  \rnd{c}$ are also jointly gaussian.
  Furthermore $\E[\ra (\rb-  \rnd{c})] = \E[\ra \rb] - \E[\ra \rnd{c}] = \cov(\ra,\rb) - \cov(\ra,\rb) = 0$. Thus $\ra$ and $\rb - \rnd{c}$ are uncorrelated and being gaussian are also independent.
  By linear combination of $\ra$, $\rnd{c} $ and $\rb - \rnd{c}$ are also independent.
  This allows in the first step, to decompose $\rb$ into two independent gaussian random variables: $\rb =\rnd{c}  + \rb -  \rnd{c}$.
  
  \end{enumerate} 
  
 \item (Oxford Dataset) In this problem, we will compute an estimator for rainfall in Oxford as a function of the maximum temperature. \verb|oxford.zip| contains the support code for the problem and the dataset. \verb|regression.py| within \verb|oxford.zip| reads the dataset and splits it into train, validation and test sets. We parameterize our estimator for rainfall($y$) from maximum temperature($x$) as 
  \begin{equation*}
f_a(x)=\begin{cases}
          w_1x + b_1 \quad &\text{if} \, x< a \\
         w_2x + b_2 &\text{if} \, x \geq a \\
     \end{cases}
 \end{equation*}
 $w_1, w_2, b_1$ and $b_2$ are estimated by minimizing the mean squared error on the training dataset. 
 \begin{enumerate}
 \item Complete \verb|split_and_plot()| in \verb|regression.py| to fit two different linear function for a given value of threshold $a$. The function will generate a plot of the fit overlaid on a scatter plot of the validation data. Report the plot generate by the function for different values of $a$ defined in \verb|main()|. You are welcome to try other values of $a$, but please make sure that you report the plots generated for all values of $a$ defined in \verb|main()|. 
 \item Choose the best estimator $f_{a'}(x)$ according to the validation error. Fill in the rest of \verb|main()| function to fit a single linear estimator on the entire dataset. Compare the fit and error values of $f_{a'}(x)$ with the single linear estimator fit on the training set on the held out test set.  Report the plot generated by this section. 
 \end{enumerate}
 We do not require you to include your code in the report. You can choose to include it or not include it. 
\end{enumerate}
\end{document}
