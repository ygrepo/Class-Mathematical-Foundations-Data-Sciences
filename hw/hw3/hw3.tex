\documentclass[12pt,twoside]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,graphicx,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm,multirow,wasysym,appendix}
\usepackage{array,subcaption} 
% \usepackage[small,bf]{caption}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage[psamsfonts]{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{textcomp}
\usepackage[left=0.8in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage{mathtools}
\usepackage{minted}

\input{macros}
\begin{document}

\noindent DS-GA.1013 Mathematical Tools for Data Science \\
Homework 3 \\
Yves Greatti - yg390\\

\begin{enumerate}
\item (PCA and linear regression) Consider a dataset of $n$ 2-dimensional data points $x_1,\ldots,x_n \in \R^2$. Assume that the dataset is centered. Our goal is to find a line in the 2D space that lies \emph{closest} to the data. First, we apply PCA and consider the line in the direction of the first principal direction. Second, we fit a linear regression model where $x_i[1]$ is a feature, and $x_i[2]$ the corresponding response. Are these lines the same? Describe each line in terms of the quantity it minimizes geometrically (e.g. sum of some distance from the points to the lines).\\

	These lines are not the same. The linear regression model approximates the response $x_i[2]$ by  finding the feature point  $x_i[1]$  the closest to the response point $x_i[2]$ by projecting the response point $x_i[2]$
	onto the hyperspace $x[1]^T \beta $. For the OLS estimator this projection is orthogonal: $\beta_{\text{OLS}} =  \arg \min_{\beta \in \R} \| x_i[2] - \beta x_i[1] \|_2$.
	
	PCA finds the principal directions by maximizing the total variance of the sample covariance matrix.
	Let $u_1$ the first principal direction, $u_1 = \arg \max_{\|v\|_2} \var(\mathcal{P}_v \mathcal{X})$, where $\mathcal{X}$ is the feature matrix: $\mathcal{X} \in \R^{2 \times n}, \mathcal{X} = [ x_1,\ldots,x_n ]$.
	Consider one datapoint $x_i, i=1, \ldots,n$, applying the Pythagorean theorem we can show that the norm of the total variance squared is: 
	$\|x_i\|^2 = \| \mathcal{P}_{u_1} x_i\|^2 + \| x_i - \mathcal{P}_{u_1} x_i \|^2$ .
	For the given sample covariance  matrix $\mathcal{X} \mathcal{X}^T$, PCA tries to maximize the total variance or equivalently mimize the loss variance by projecting orthogonally the data  $\mathcal{X}$ onto the  line  in the direction of the first principal direction.
	

 \item (Heartbeat) We are interested in computing the best linear estimate of the heartbeat of a fetus in the presence of strong interference in the form of the heartbeat of the baby's mother. To simplify matters, let us assume that we only want to estimate the heartbeat at a certain moment. We have available a measurement from a microphone situated near the mother's belly and another from a microphone that is away from her belly. We model the measurements as
\begin{align}
\rx[1] & = \rb + \rnd{m} + \rnd{z_1}\\
\rx[2] & = \rnd{m} + \rnd{z_2},
\end{align}
where $\rb$ is a random variable modeling the heartbeat of the baby, $\rnd{m}$ is a random variable modeling the heartbeat of the mother, and $\rnd{z}_1$ and $\rnd{z}_2$ model additive noise. From past data, we determine that $\rb$, $\rnd{m}$, $\rnd{z}_1$, and $\rnd{z}_2$ are all zero mean and uncorrelated with each other. The variances of $\rb$, $\rnd{z}_1$ and $\rnd{z}_2$ are equal to $1$, whereas the variance of $\rnd{m}$ is much larger, it is equal to $10$.
\begin{enumerate}
\item Compute the best linear estimator of $\rb$ given $\rx[1]$ in terms of MSE, and the corresponding MSE. Describe in words what the estimator does. 

We have shown in class that centering the variables does not change the MSE, so we want to estimate 
MSE = $\min_\beta \E[(\rb - \beta \rx[1])^2] = \beta^2 \var(\rx[1]) + \var(\rb) -2 \beta \cov(\rx[1], \rb)$.
\begin{align*}
	 \var(\rx[1])		&= 	\var(\rb + \rnd{m} + \rnd{z_1}) \\
	 				&= 	\var(\rb) + \var(\rnd{m} )	+ \var(\rnd{z_1})	\\
					&=	1 + 10 + 1	=12	 \\
	\cov(\rx[1], \rb)		&=	\E[\rx[1] \rb]	\\
					&= 	\E[ (\rb + \rnd{m} + \rnd{z_1}) \rb] = \E[\rb^2] = 1
\end{align*}
where we have used  that $\rb$, $\rnd{m}$, $\rnd{z}_1$ are all zero mean and uncorrelated with each other.

MSE = $12 \beta^2 -2 \beta + 1$, it is a convex quadratic function with respect to $\beta$, so we can set the derivative to zero to find the minimum: $\beta^* = \frac{1}{12}$.
MSE$_{\beta^*} = \frac{11}{12} = 0.91$. This estimator predicts the heartbeat of the baby using the measurement $\rx[1]$ from the microphone situated near  the mother's belly.

\item Compute the best linear estimator of $\rb$ given $\rx$ in terms of MSE, and the corresponding MSE. Describe in words what the estimator does. 

\begin{proof}

$\E[\rb]=0$, $\E[\rx[1]] = \E[\rb + \rnd{m} + \rnd{z_1}] = \E[\rb] + \E[\rnd{m} ] + \E[\rnd{z_1}]  = 0$, $\E[\rx[2]] = \E[\rnd{m} + \rnd{z_2}] = \E[\rnd{m}] + \E[\rnd{z_2}] = 0$.
$\rb$ is a zero-mean random variable and $\rx$ is a zero mean random vector with a full covariance matrix. 

From theorem 2.3 of the linear regression notes, the MSE of this estimator is equal to $\var(\rb) - \Sigma_{\rb\rx}^T \Sigma_{\rx}^{-1} \Sigma_{\rb\rx}$.

$\rb$, $\rnd{m}$, $\rnd{z}_1$, and $\rnd{z}_2$ are all zero mean and uncorrelated with each other thus

\begin{align*}
	\cov(\rx[1], \rx[2]) 	&= 	\E[\rx[1]  \rx[2]] - \E[\rx[1]] \E[\rx[2]] = \E[\rx[1]]  \E[\rx[2]] = \E[\rnd{m}^2] = \var(\rnd{m}) = 10	\\
	 \var(\rx[2])		&=	\var(\rnd{m} + \rnd{z_2}) = \var(\rnd{m}) + \var(\rnd{z_2}) 	\\
	 				&=	10 + 1 = 11	\\
	\cov(\rb, \rx)		&=	\E[\rb \rx] - \E[\rb] \E[\rx] = \E[\rb \rx] \\
					&= 	[\E[\rx[1] \rb] \; \E[\rx[2] \rb]]^T = [1 \; 0]^T
\end{align*}



This gives us:
\begin{align*}
	\Sigma_{\rx} &= 
	\begin{bmatrix}
		\var(\rx[1])	&			\cov(\rx[1], \rx[2]) \\
		\cov(\rx[2], \rx[1])	&	\var(\rx[2])		\\
	\end{bmatrix}
	=
	\begin{bmatrix}
		12	&	10 \\
		10	&	11 
	\end{bmatrix} \\
	\Sigma_{\rx}^{-1} &= 
	\begin{bmatrix}
		\frac{11}{32}	&	-\frac{5}{16} \\
		-\frac{5}{16}	&	-\frac{3}{8}
	\end{bmatrix} \\
	\Sigma_{\rb\rx}^T \Sigma_{\rx}^{-1} \Sigma_{\rb\rx} &=
	[1 \; 0] 	\begin{bmatrix} \frac{11}{32} \\ -\frac{5}{16} \end{bmatrix} = \frac{11}{32}
\end{align*}
Hence MSE = $ 1 - \frac{11}{32} = \frac{21}{32} = 0.65$. The second estimator provides a better estimation of the heartbeat of the baby by using both microphones.

\end{proof}

\end{enumerate}

\item (Gaussian minimum MSE estimator) In this problem we derive the minimum MSE estimator of a random variable $\rnd{b}$ given another random variable $\rnd{a}$ when both are jointly Gaussian. To simplify matters we assume the mean of both random variables is zero. 
  \begin{enumerate}
  \item Let us define
  \begin{align}
  \rnd{c} := \frac{\cov(\ra,\rb)}{\var(\ra)}\ra.
  \end{align}
  Consider the decomposition of $\rnd{b}$ into the sum of $\rnd{c} $ and $\rb - \rnd{c}$. Provide a geometric interpretation of this decomposition. \\
  This decomposition is the orthogonal projection of $\rnd{b}$ into a vector in the span of $\rnd{a}$: $\rnd{c}$ and a vector orthogonal to this hyperspace:  $\rb - \rnd{c}$.
  
  \item Compute the conditional expectation of $\rnd{c} $ given $\ra=a$ for a fixed $a \in \R$.
  $$\E[\rnd{c} | \rnd{a}= a] = \E[\frac{\cov(\ra,\rb)}{\var(\ra)}\ra | \ra= a] = \frac{\cov(\ra,\rb)}{\var(\ra)}\ra$$
  
  \item Compute the conditional expectation of $\rb - \rnd{c}$ given $\ra=a$ for a fixed $a \in \R$. (Hint: Start by computing the covariance between $\rb - \rnd{c}$ and $\ra$.)
  First few results
  \begin{align*}
  	\cov(\ra, \rb)	&=	 \E[\ra \; \rb]	- \E[\ra] \; \E[\rb]	\\
				&=	 \E[\ra \; \rb] - 0 = \E[\ra \; \rb] 		\\
	\var(\ra)		&=	 \E[\ra^2] - \E[\ra]^2 =  \E[\ra^2]		\\
	\E[\rb  - \rnd{c}]	&= 	 \E[ \rb  - \frac{\cov(\ra,\rb)}{\var(\ra)}\ra] \\
				&=	 \E[ \rb] - \frac{\cov(\ra,\rb)}{\var(\ra)} \E[\ra] = 0 
  \end{align*}
  Therefore $$\cov(\rb  - \rnd{c}, \ra)   = \E[ \rb  \; \ra] -  \frac{\cov(\ra,\rb)}{\var(\ra)} \E[\ra^2]  = \cov(\ra, \rb) - \cov(\ra, \rb) = 0$$
  Thus  $\rb  - \rnd{c}$ and $\ra$ are uncorrelated thus $\E[ \rb - \frac{\cov(\ra,\rb)}{\var(\ra)} \ra | \ra= a ] = \E[ \rb - \frac{\cov(\ra,\rb)}{\var(\ra)} \ra] = \E[ \rb] -  \frac{\cov(\ra,\rb)}{\var(\ra)}  \E[\ra] = 0$.
  
  \item Prove that the minimum MSE estimator of $\rnd{b}$ given $\ra=a$ for a fixed $a \in \R$ is linear. 
  
  Using the problem assumptions, and theorem 2.1 from our class, the minimum MSE estimator of $\rnd{b}$ given $\ra=a$ for a fixed $a \in \R$ is given by:
  \begin{align*}
  	\E[\rb | \ra = a]		&=	\E[\rb-  \rnd{c} +  \rnd{c} | \ra= a]	\\
					&=	\E[\rb-  \rnd{c} | \ra= a] + \E[\rnd{c} | \ra= a] \\
					&=	\frac{\cov(\ra,\rb)}{\var(\ra)} \ra
  \end{align*}
  which proves that the  minimum MSE estimator of $\rnd{b}$ given $\ra=a$ for a fixed $a \in \R$ is linear.
  
  \item What step of the proof fails for non-Gaussian random variables?\\
  The first step.
  
  Since $\ra$ and $\rb$ are gaussian random variables then $\ra$ and $\rb -  \rnd{c}$, where $\rnd{c} := \frac{\cov(\ra,\rb)}{\var(\ra)}\ra$, are also jointly gaussian.
  Furthermore $\E[\ra (\rb-  \rnd{c})] = \E[\ra \rb] - \E[\ra \rnd{c}] = \cov(\ra,\rb) - \cov(\ra,\rb) = 0$. Thus $\ra$ and $\rb - \rnd{c}$ are uncorrelated and being gaussian are also independent.
  By linear combination of $\ra$, $\rnd{c} $ and $\rb - \rnd{c}$ are also independent.
  This allows in the first step, to decompose $\rb$ into two independent gaussian random variables: $\rb =\rnd{c}  + \rb -  \rnd{c}$.
  
  \end{enumerate} 
  
 \item (Oxford Dataset) In this problem, we will compute an estimator for rainfall in Oxford as a function of the maximum temperature. \verb|oxford.zip| contains the support code for the problem and the dataset. \verb|regression.py| within \verb|oxford.zip| reads the dataset and splits it into train, validation and test sets. We parameterize our estimator for rainfall($y$) from maximum temperature($x$) as 
  \begin{equation*}
f_a(x)=\begin{cases}
          w_1x + b_1 \quad &\text{if} \, x< a \\
         w_2x + b_2 &\text{if} \, x \geq a \\
     \end{cases}
 \end{equation*}
 $w_1, w_2, b_1$ and $b_2$ are estimated by minimizing the mean squared error on the training dataset. 
 \begin{enumerate}
 \item Complete \verb|split_and_plot()| in \verb|regression.py| to fit two different linear function for a given value of threshold $a$. The function will generate a plot of the fit overlaid on a scatter plot of the validation data. Report the plot generate by the function for different values of $a$ defined in \verb|main()|. You are welcome to try other values of $a$, but please make sure that you report the plots generated for all values of $a$ defined in \verb|main()|. 

 \medskip

	In the function  \verb|split_and_plot()| of \verb|regression.py|, we follow these steps:
	\begin{enumerate}[(1)]
		\item Split the training set for maximum temperatures into two data sets less or greater than temperature a ($\le$ or $\ge$). 
		We do the same for the rainfall datasets, splitting the rainfall dataset related to temperatures  less or greater than temperature a  ($\le$ or $\ge$). 
		\item Fit two linear models, one with the data related to temperatures less or equal to $a$ and the related rainfall data points, and one model 
		with the training data corresponding to temperatures greater or equal to $a$ and related rainfall data points. 
		\item We obtain two sets of prediction values on relevant points of the grid using linear fit with points max\_temp $\le a$ and max\_temps $\ge a$.
		\item We then compute the training mean squared errors for the two linear models, 
		using the relevant training data points (model 1 using max\_temp $\le a$ and  model 2 using max\_temps $\ge a$), between their predictions for rainfalls
		and the relevant rainfall data split on $a$ with the same inequalities. 
		The total training mse is a weighted average of the two training mean squared errors: 
		\begin{align*}
			& \frac{n1 * \text{training mse  1} + n_2  * \text{training mse 2}} {n_1 + n_2} \\
			& n_1: \text{ number of samples in data set used by linear model 1} \\
			& n_2: \text{ number of samples in data set used by linear model 2}
		\end{align*}
		\item  Following the same logic, we compute the total validation mse using the two linear model and the max. temperatures and rainfall validation data points.
		\item The lowest validation mse is $1134.86$ and it is obtained with two linear models using two datasets divided by a temperature $a=20$.

	\end{enumerate}	
 	We also tried various linear estimators: linear regression, stochastic gradient descent, and with $l2$ regularization: ridge and Bayesian ridge regressors, 
	using different values of $a$ as split points to train the estimators.
	Out-of-the box the linear regression model had the best validation accuracy or the lowest validation error (see following plots for various $a$).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=200pt]{figures/a=4.pdf}
		\caption{Linear regression of rain fall as response of max. temperatures with $a=4$}
		\label{fig1}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=200pt]{figures/a=8.pdf}
		\caption{Linear regression of rain fall as response of max. temperatures with $a=8$}
		\label{fig2}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=200pt]{figures/a=12.pdf}
		\caption{Linear regression of rain fall as response of max. temperatures with $a=12$}
		\label{fig3}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=200pt]{figures/a=16.pdf}
		\caption{Linear regression of rain fall as response of max. temperatures with $a=16$}
		\label{fig4}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=200pt]{figures/a=20.pdf}
		\caption{Linear regression of rain fall as response of max. temperatures with $a=20$}
		\label{fig5}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=200pt]{figures/a=24.pdf}
		\caption{Linear regression of rain fall as response of max. temperatures with $a=24$}
		\label{fig5}
	\end{figure}
	
 \item Choose the best estimator $f_{a'}(x)$ according to the validation error. Fill in the rest of \verb|main()| function to fit a single linear estimator on the entire dataset. Compare the fit and error values of $f_{a'}(x)$ with the single linear estimator fit on the training set on the held out test set.  Report the plot generated by this section. 
 
 	Having identified the best value $a$ ($a=20$) to split the training data, we then train two linear models using this value of $a$ by splitting the entire training dataset between
	max.temperatures $\le a$ and max. temperatures $\ge a$ and fitting with the split on the rainfalls using the same value of $a$.
	We then train a linear model on the entire training set using the training sets of maximum temperatures and rainfall data.
	We observe that our best estimator $f_{a'}(x)$ has better accuracy or equivalently lower mean squared error: $1487.4$ vs. $1680$ for the single linear model,
	Splitting the data into two datasets allowed us to make better predictions reducing the mse by $11$\%.
	
 	\begin{figure}[H]
		\centering
		\includegraphics[width=200pt]{figures/test_comparison.pdf}
		\caption{Comparison between a single linear estimator fitted on the whole dataset and the  the best linear estimator $f_{a'}$}
		\label{fig7}
	\end{figure}


 \end{enumerate}
 	
 We do not require you to include your code in the report. You can choose to include it or not include it. 
\end{enumerate}
\end{document}
